<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tf.about.annotate API documentation</title>
<meta name="description" content="Manual Annotation â€¦" />
<!-- integrity SRI from https://cdnjs.com/libraries/10up-sanitize.css/11.0.1 -->
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css"
integrity="sha512-kcbluZFacWN57NgWZ4aH6eUMBEaTyErFhIFD3y5qYZbKuuyImH0K/AKsBbfXlivh2z5C+3IDTIhI11YmKomzmA=="
crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css"
integrity="sha512-uVeAgzAmieLUTGba0qr9vXQgVD7fko2kcbYIKIraXUIDg9iJLxveTFUrg3DJhqn3cAf3HFDbgmhq0eGko5wEAA=="
crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tf.about.annotate</code></h1>
</header>
<section id="section-intro">
<h1 id="manual-annotation">Manual Annotation</h1>
<p>Named Entity Recognition is a task for which automated tools exist.
But in a smallish, well-known corpus, such tools tend to create more noise than signal.
If you know which names to expect and in what forms they occur in the corpus,
it might be easier to mark them by hand.</p>
<p>TF contains a tool to help you with that. The basic thing it does is
to find all occurrences of a pattern you specify, and then assign an identifier and
a kind to all those occurrences.</p>
<p>For example, it finds all <code>Ernesto di Nassau</code> occurrences and it assigns
identity <code>ernesto.di.nassau</code> and kind <code>PER</code> to them.</p>
<p>There are two interfaces. One runs inside the TF browser, and it
enables you to search and mark repeatedly, to apply additions and
deletions of named entities to restricted sets of occurrences, so that you can
make individual exceptions to the rules.</p>
<p>The other is an API, that you can call, typically from inside a Jupyter notebook,
by which you can automate things to the next level.
You can prepare a spreadsheet with entities and surface forms and have the tool
<em>execute</em> that spreadsheet against the corpus.</p>
<p>Here is what the browser interface looks like:</p>
<p><img alt="browser" src="../images/Annotate/browser.png"></p>
<h1 id="work-in-progress">Work-in-progress</h1>
<p>This is a tool where you can add entity annotations to a corpus.
These annotations consist of a new nodes of a new type (<code>ent</code>, or <code>entity</code>) and
features that give information about those nodes.</p>
<h2 id="supported-corpora">Supported corpora</h2>
<p>This tool is being developed against the
<a href="https://github.com/HuygensING/suriano"><code>HuygensING/suriano</code> corpus</a>.
Yet it is meant to be usable for all TF corpora. No corpus knowledge is baked in.
If corpus specifics are needed, it will be fetched from the already present TF
configuration of that corpus.
And where that is not sufficient, details are put in <code>ner/config.yaml</code>, which
can be put next to the corpus.</p>
<p>I have tested it for the
<a href="https://github.com/ETCBC/bhsa"><code>ETCBC/bhsa</code> (Hebrew Bible) corpus</a>, and the
machinery works.
But it remains to be seen whether the tools is sufficiently <em>ergonomic</em> for that
corpus.</p>
<p>See also the following Jupyter Notebooks that show the work-in-progress:</p>
<ul>
<li>
<p>Suriano</p>
<ul>
<li><a href="https://nbviewer.org/github/HuygensING/suriano/blob/main/programs/nerTest.ipynb">basic annotation API</a></li>
<li><a href="https://nbviewer.org/github/HuygensING/suriano/blob/main/programs/ner.ipynb">using a spreadsheet with instructions</a></li>
</ul>
</li>
<li>
<p>BHSA</p>
<ul>
<li><a href="https://nbviewer.org/github/ETCBC/bhsa/blob/master/programs/nerTest.ipynb">basic annotation API</a></li>
<li><a href="https://nbviewer.org/github/ETCBC/bhsa/blob/master/programs/ner.ipynb">using a spreadsheet with instructions</a></li>
<li><a href="https://nbviewer.org/github/ETCBC/bhsa/blob/master/tutorial/cookbook/nerByTheBook.ipynb">cookbook recipe</a></li>
</ul>
</li>
</ul>
<h2 id="ergonomics-of-annotation">Ergonomics of annotation</h2>
<p>We try to reduce the work of manual annotations as much as possible.
It is a balancing act between automating as much as possible, but not so much that
you miss the fine points in your corpus.</p>
<p>We need to gather experience in order to arrive at a truly usable tool.</p>
<p>We are going to mark up the
<a href="https://github.com/HuygensING/suriano"><code>HuygensING/suriano</code> corpus</a> in this way and hope
to acquire a lot of experience in the process.</p>
<h2 id="delivery-of-annotation-data">Delivery of annotation data</h2>
<p>But how exactly are those new nodes and features delivered?</p>
<p>This is work in progress!</p>
<p>Currently, TF supports data modules on top of a corpus, provided the modules
consist of features that annotate the existing nodes in the corpus.</p>
<p>Here we have a new situation: we not only have new features, but also a new node type.</p>
<p>TF has not (yet) a module system by which you can invoke such data modules on top of
an existing TF dataset.</p>
<p>What do we have, though?</p>
<p>TF has already functions to <em>add</em> new types with their features to a dataset:
<code><a title="tf.dataset.modify" href="../dataset/modify.html">tf.dataset.modify</a></code>. This will create a completely new and separate data set out of
the existing dataset and the new nodes and features.</p>
<p>We deliver the annotation data as TSV files, where each line specifies
a new (entity) node, and the columns specify the values of the entity features for
that node, plus the slots that are linked to that node.</p>
<p>With this information in hand, it is possible</p>
<ul>
<li>to call the <code><a title="tf.dataset.modify" href="../dataset/modify.html">tf.dataset.modify</a></code> function <strong>or</strong></li>
<li>work in a Jupyter notebook and use the entity data in whatever way you like.</li>
</ul>
<p>In the future I intend to broaden the concept of data module to modules that introduce
new node types. In order to do that I have to write code for TF to include
the module data in the existing <code>otype</code> and <code>oslots</code> features and, most of all,
to generate adapted computed features such as <code>__levup__</code> and <code>__levdown__</code>.
See <code><a title="tf.core.prepare" href="../core/prepare.html">tf.core.prepare</a></code> .</p>
<h1 id="concepts">Concepts</h1>
<h2 id="entities">Entities</h2>
<p>An <em>entity</em> is a thing in the real world that is referenced by specific pieces of
text in the corpus. Think of persons, places, organizations. </p>
<p>We mark up those text occurrences by creating nodes for those locations in the corpus
and assigning feature values to the features <code>eid</code> (entity identifier) and <code>kind</code>
(entity kind such as <code>PER</code>, <code>LOC</code>, <code>ORG</code>).</p>
<h2 id="how-entities-exist-in-a-corpus">How entities exist in a corpus</h2>
<p>Your corpus may already have entities, marked up by an automatic tool such as
Spacy (see <code><a title="tf.tools.myspacy" href="../tools/myspacy.html">tf.tools.myspacy</a></code>). In that case there is already a node type
<code>ent</code> and features <code>eid</code> and <code>kind</code>.</p>
<p>When you make manual annotations, the annotations are saved as TSV files.
In order to create nodes for them in your corpus, you can use those TSV files,
read off the feature information, and invoke <code><a title="tf.dataset.modify" href="../dataset/modify.html">tf.dataset.modify</a></code> function to
generate new nodes and add them to your corpus.</p>
<p>In a next iteration, we'll include a function that will do this for you.</p>
<h2 id="occurrences-and-identifiers">Occurrences and identifiers</h2>
<p>Entity nodes mark the text occurrences that refer to outside entities in the world.
Different occurrences have different entity nodes, but they may refer to the same
entity in the world. It is the entity identifier that unifies the various occurrences
that refer to the same entity in the world.
We do not have nodes in the corpus that correspond 1-1 to the real world entities.
The entity nodes correspond to the occurrences.</p>
<p>The same occurrence may have multiple entity nodes.
For example, if an occurrence <code>Amsterdam</code> refers to a location, a city council,
and a ship at the same time, you might mark it up as</p>
<ul>
<li><code>('amsterdam', 'LOC')</code></li>
<li><code>('amsterdam', 'ORG')</code></li>
<li><code>('amsterdam', 'SHIP')</code></li>
</ul>
<p>In fact, an entity in the real word is not solely identified by the <code>eid</code> feature,
but by the <em>combination</em> of the <code>eid</code> and <code>kind</code> features.</p>
<p>Of course, you are free to make the identifiers distinct if the same name refers
to entities of different kinds.</p>
<h2 id="entity-sets">Entity sets</h2>
<p>When you are in the process of marking entities, you create an entity <em>set</em>.
You can give this a name, and the data you create will be stored under that name.</p>
<h2 id="for-annotators">For annotators</h2>
<p>Go to the manual for annotating in the TF browser:
<code><a title="tf.about.annotateBrowser" href="annotateBrowser.html">tf.about.annotateBrowser</a></code></p>
<h2 id="for-programming-annotators">For programming annotators</h2>
<p>Go to the manual for annotating in in a Jupyter Notebook, using the API:
<code><a title="tf.browser.ner.ner" href="../browser/ner/ner.html">tf.browser.ner.ner</a></code>
Or see this
<a href="https://nbviewer.jupyter.org/github/HuygensING/suriano/blob/main/programs/ner.ipynb">example notebook</a>.</p>
<h2 id="for-corpus-maintainers">For corpus maintainers</h2>
<p>This tool needs additional input data and produces additional output data.</p>
<p>The <em>input</em> data can be found in the directory <code>ner</code> next to the actual <code><a title="tf" href="../index.html">tf</a></code> directory
from where the program has loaded the corpus data.</p>
<p>Depending on how you invoke TF, this can be in a clone of the
repository of the corpus, or an auto-downloaded copy of the data.</p>
<p>If you work with the corpus in a local clone, you'll find it under
<code>~/github/HuygensING/suriano</code> (in this example).</p>
<p>If you work in an auto-downloaded copy of the corpus, it is under
<code>~/text-fabric-data/github/HuygensING/suriano</code> (in this example).</p>
<p>Note that</p>
<ul>
<li>the part <code>github</code> may be another back-end in your situation, e.g. <code>gitlab</code> or
<code>gitlab.huc.di.nl</code>;</li>
<li>the part <code>HuygensING</code> may be another organization, e.g. <code>annotation</code>, or wherever
your corpus is located;</li>
<li><code>suriano</code> might also be another repo such as <code>descartes</code>, or where ever
your corpus is located.</li>
</ul>
<p>The <em>output</em> data can be found a directory <code>_temp/ner</code> where the <code>_temp</code>
directory is located next to the <code>ner</code> directory that holds the input data.</p>
<h3 id="input-data">Input data</h3>
<p>There are several bits of information needed to set up the annotation tool.
They are corpus specific, so they must be specified in a YAML file in the corpus
repository.</p>
<p>As an example, we refer to the
<a href="https://github.com/HuygensING/suriano">Suriano corpus</a>.</p>
<ul>
<li><code>config.yaml</code> in directory <code>ner</code>;</li>
<li>optional Excel sheets in directory <code>ner/sheets</code> with instructions to
bulk-annotate entities.</li>
</ul>
<p>Concerning <code>ner/config.yaml</code>: it has the following information:</p>
<ul>
<li>
<p><code>entityType</code>: the node type of entities that are already in the corpus,
possibly generated by a tool like Spacy;</p>
</li>
<li>
<p><code>entitySet</code>: a name for the pre-existing set of entities;</p>
</li>
<li>
<p><code>bucketType</code>: the annotation works with paragraph-like chunks of text
of your corpus. These units are called <em>buckets</em>.
Here you can specify which node type must be taken as the buckets.</p>
</li>
<li>
<p><code>features</code>: the features that contain essential information about
the entities. Currently, we specify only 2 features. You may rename these
features, but we advise not modify the number of features.
Probably, in later releases, you'll have more choice here.</p>
</li>
<li>
<p><code>keywordFeatures</code>: some features have a limited set of values, e.g.
the kind of entity. Those features are mentioned under this key.</p>
</li>
<li>
<p><code>defaultValues</code>: provide default values for the keyword features.
The tool also provides a default for the first feature, the entity
identifier, basically a lower case version of the full name where
the parts of the name are separated by dots.</p>
</li>
<li>
<p><code>spaceEscaped</code>: set this to True if your corpus as tokens with spaces in it.
You can then escape spaces with <code>_</code>, for example in spreadsheets where you
specify annotation instructions.</p>
</li>
<li>
<p><code>transform</code>: the tool can read a spreadsheet with full names and per name
a list of occurrences that should be marked as entities for that name.
When the full name is turned into an identifier, the identifier might
become longer than is convenient. Here you can specify a replacement
table for name parts. You can use it to shorten or repress certain
name parts that are very generic, such as <code>de</code>, <code>van</code>, <code>von</code>. </p>
</li>
</ul>
<p>Concerning the Excel sheets in <code>ner/sheets</code>:</p>
<ul>
<li>
<p>they can be read by <code><a title="tf.browser.ner.ner.NER.readInstructions" href="../browser/ner/ner.html#tf.browser.ner.ner.NER.readInstructions">NER.readInstructions()</a></code>;</p>
</li>
<li>
<p>you might need to <code>pip install openpyxl</code> first;</p>
</li>
<li>
<p>only the first two columns of the sheet are read, the first column is
expected to have the full names, the second column is a list of
surface forms that trigger the marking of an entity with this name, separated
by semicolons.</p>
</li>
</ul>
<h3 id="good-practice">Good practice</h3>
<p>All NER input data (configuration file and Excel sheets) should reside in the
repository.
The corresponding TF app should specify in its <code>app/config.yaml</code>,
under <code>provenanceSpecs</code>:</p>
<ul>
<li><code>extraData: ner</code></li>
</ul>
<p>When you make a new release on GitHub of the repo, do not forget to run</p>
<pre><code class="language-python">A = use(&quot;HuygensING/suriano:clone&quot;, checkout=&quot;clone&quot;)
A.zipAll()
</code></pre>
<p>Then pick up the new <code>~/Downloads/github/HuygensING/suriano/complete.zip</code>
and attach it to your new release on GitHub.</p>
<p>Then other users can invoke the corpus by</p>
<pre><code class="language-python">A = use(&quot;HuygensING/suriano&quot;)
</code></pre>
<p>This way there is no need to clone the repository first.
TF will auto-download the corpus, including the <code>ner</code> input data.</p>
<p>Then multiple people can work on annotation tasks on their local computers.</p>
<h3 id="output-data">Output data</h3>
<p>The results of all your annotation actions will end up in the
<code>_temp/ner/</code><em>version</em> folder in your corpus, where <em>version</em> is the
TF version of your corpus.
The output data is tightly coupled to a specific version of the corpus.</p>
<div class="admonition caution">
<p class="admonition-title">Versioning</p>
<p>If a new versions of the corpus is published, the generated annotations
do not automatically migrate to the new version.
TF has tools to perform those migrations, but they are not fully automatic.
Whenever a new version of the corpus is produced, the producer should also
generate a mapping file from the nodes of the new corpus to the old corpus.
That will enable the migration of the annotations.
See <code><a title="tf.dataset.nodemaps" href="../dataset/nodemaps.html">tf.dataset.nodemaps</a></code> .</p>
</div>
<h3 id="recommended-practice">Recommended practice</h3>
<p>When the annotators are done, you need to ask them to dig out their data files and send
them to you.</p>
<p>Then you can turn that data into new entity nodes and features and merge
them into the corpus.
This is not trivial, it involves using <code><a title="tf.dataset.modify" href="../dataset/modify.html">tf.dataset.modify</a></code>.
I intend to write functions to make this task easier.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/45319d66c4a6e30233fc91a7f636c02eb770f1b4/tf/about/annotate.py#L1-L3" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
.. include:: ../docs/about/annotate.md
&#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<p><a href="https://github.com/annotation" title="annotation on GitHub"><img src="../../tf/images/tf-small.png" alt="annotation"></a></p>
<p><a href="../../tf/index.html">tf home</a> -
<a href="../../tf/cheatsheet.html">cheat sheet</a> -
<a href="https://github.com/annotation/text-fabric" title="GitHub repo"><img src="../../tf/images/GitHub_Logo.png" alt="GitHub" width="50"></a></p>
</p>
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#manual-annotation">Manual Annotation</a></li>
<li><a href="#work-in-progress">Work-in-progress</a><ul>
<li><a href="#supported-corpora">Supported corpora</a></li>
<li><a href="#ergonomics-of-annotation">Ergonomics of annotation</a></li>
<li><a href="#delivery-of-annotation-data">Delivery of annotation data</a></li>
</ul>
</li>
<li><a href="#concepts">Concepts</a><ul>
<li><a href="#entities">Entities</a></li>
<li><a href="#how-entities-exist-in-a-corpus">How entities exist in a corpus</a></li>
<li><a href="#occurrences-and-identifiers">Occurrences and identifiers</a></li>
<li><a href="#entity-sets">Entity sets</a></li>
<li><a href="#for-annotators">For annotators</a></li>
<li><a href="#for-programming-annotators">For programming annotators</a></li>
<li><a href="#for-corpus-maintainers">For corpus maintainers</a><ul>
<li><a href="#input-data">Input data</a></li>
<li><a href="#good-practice">Good practice</a></li>
<li><a href="#output-data">Output data</a></li>
<li><a href="#recommended-practice">Recommended practice</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tf.about" href="index.html">tf.about</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://pure.knaw.nl/portal/en/persons/dirk-roorda">Dirk Roorda</a>
<a href="https://huc.knaw.nl"><img alt="HuC" src="../../tf/images/huc.png" width="200" alt="Humanities Cluster"></a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>