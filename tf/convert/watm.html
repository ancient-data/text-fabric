<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tf.convert.watm API documentation</title>
<meta name="description" content="Export to Web Annotation Text Model â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-GjqubOlYA6/wOZV7F31o2t4ogk4JGDGFa6XmL+BfAG0=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-qDPJ4nH38AgHyK3U6cvI6DBifA+7hPBo8mVoH6pyN9c=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tf.convert.watm</code></h1>
</header>
<section id="section-intro">
<p>Export to Web Annotation Text Model</p>
<h1 id="the-general-idea">The general idea</h1>
<p>This module can export a TF corpus to WATM (Web Annotation Text Model),
which is the input format of the suite of systems developed by Team Text for
serving text plus annotations over the web.</p>
<p>The idea of WATM is, like the idea of Text-Fabric, to untangle the text from its
markup. Everything outside the text itself is coded in annotations.</p>
<p>Annotations look a lot like TF features, but they are a bit more general.
Annotations can also annotate annotations, not only pieces of text.</p>
<p>We need this extra generality, because unlike TF, WATM does not have a concept
of node. The only parallel are the slot nodes of TF, which corresponds to the
tokens of the text in WATM.</p>
<p>Every node in TF is linked to a set of slot nodes.
As such it can be mapped to an annotation to the corresponding tokens.
Features of such nodes can be mapped to annotations to annotations.</p>
<p>TF also has edges. These can be mapped to WATM annotations whose targets are
pairs: one for the thing the edge is <em>from</em>, and one for the thing the edge is <em>to</em>.
These things are typical annotations that correspond to TF nodes, since TF edges
are links between TF nodes.</p>
<p>If the TF dataset itself is the result of converting an XML file (e.g TEI or
PageXML), then there is a further correspondence between the XML and the TF:</p>
<ul>
<li>elements translate into nodes; element tags translate into node types;</li>
<li>attributes translate into features; values of attributes translate into
values of features.</li>
</ul>
<p>In our terminology below we assume that the TF data comes from XML files,
but this is not essential. Whenever we talk about <em>elements</em> and <em>tags</em>,
you may read <em>nodes</em> and <em>node types</em> if the TF dataset does not have an XML
precursor. Likewise, for <em>attributes</em> you may read <em>features</em>.</p>
<h1 id="the-specifics">The specifics</h1>
<p>We generate tokens and annotations out of a TF dataset. Here is what we deliver
and in
what form:</p>
<ul>
<li>a bunch of files <code>text-0.json</code>, <code>text-1.json</code>: with the text segments in an array;
Each file corresponds with a top-level section in the TF dataset;</li>
<li>a bunch of files <code>anno-1.json</code>, `anno-2.json, &hellip;: all generated annotations;
We pack at most 400,000 annotations in one file, that keeps their size
below 50MB,
so that they still can live in a git directory without large file support.</li>
</ul>
<h2 id="format-of-the-text-files">Format of the text files</h2>
<p>A <code>text-i.json</code> is a JSON file with the following structure:</p>
<pre><code>{
  &quot;_ordered_segments&quot;: [
    &quot;token1 &quot;,
    &quot;token2 &quot;,
    ...
  ]
}
</code></pre>
<ul>
<li>each item in <code>_ordered_segments</code> corresponds to one token;</li>
<li>the item contains the text of the token plus the subsequent whitespace, if any;</li>
<li>if the corpus is converted from TEI, we skip all material inside the
TEI-header.</li>
</ul>
<h3 id="tokens">Tokens</h3>
<p>Tokens correspond to the slot nodes in the TF dataset.
Depending on the original format of the corpus we have the following specifics.</p>
<h4 id="tei-corpora">TEI corpora</h4>
<p>The base type is <code>t</code>, the <em>atomic</em> token.
Atomic tokens are tokens as they come from some NLP processing, except when tokens
contain element boundaries. In those cases tokens are split in fragments
between the element boundaries.</p>
<p>It is guaranteed that a text segment that corresponds to a <code>t</code> does not contain
element boundaries.</p>
<p>The original, unsplit tokens are also present in the annotations, they have
type <code>token</code>.</p>
<p>Tokens have the attributes <code>str</code> and <code>after</code>, both may be empty.</p>
<h4 id="pagexml-corpora">PageXML corpora</h4>
<p>The base type is <code>token</code>, it is available without NLP processing.</p>
<p>Tokens have the attributes <code>str</code> and <code>after</code>, both may be empty.
They may also have the attributes <code>rstr</code> and <code>rafter</code>.</p>
<ul>
<li><code>str</code> is the <em>logical</em> string value of a token, <code>after</code> is empty or a space:
what comes after the token before the next token.</li>
<li><code>rstr</code> is the raw string value of a token, <strong>when it deviates from the
logical value</strong>, otherwise no value. <code>rafter</code> analogously.</li>
</ul>
<p><strong>Example</strong></p>
<table>
<thead>
<tr>
<th>token</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>rstr</td>
<td>empty</td>
<td><code>efflagitan</code></td>
<td><code>Â¬</code></td>
<td><code>do</code></td>
<td>empty</td>
</tr>
<tr>
<td>str</td>
<td><code>improbÃ¨</code></td>
<td><code>efflagitando</code></td>
<td>empty</td>
<td>empty</td>
<td><code>tandem</code></td>
</tr>
</tbody>
</table>
<h2 id="format-of-the-annotation-files">Format of the annotation files</h2>
<p>The <code>anno-1.json</code>, <code>anno-2.json</code>, &hellip; files are JSON file with the following
structure:</p>
<pre><code>{
 &quot;a000nnn&quot;: [
  &quot;kind&quot;,
  &quot;namespace&quot;,
  &quot;body&quot;,
  &quot;target&quot;
 ],{
 ...
}
</code></pre>
<p>It is a big dictionary, keyed by annotation ids and each value is the data of
an annotation, divided in the following fields:</p>
<ul>
<li>
<p><code>kind</code>: the kind of annotation:</p>
<ul>
<li><code>element</code>: targets the text location where an <em>element</em> occurs, the body
is the element name;</li>
<li><code>pi</code>: targets the text location where a <em>processing instruction</em> occurs,
the body is the
target of the <em>pi</em>;</li>
<li><code>attribute</code>: targets an annotation (an <em>element</em> or <em>pi</em>), the body has
the shape <em>name</em><code>=</code><em>value</em>,
the name and value of the attribute in question;</li>
<li><code>node</code>: targets an individual <em>token</em> or <em>element</em> or <em>pi</em>,
the body is the TF node (a number) of that <em>token</em> / <em>element</em> / <em>pi</em>;</li>
<li><code>edge</code>: targets two node annotations, the body has the shape
<code>*name* or</code><em>name</em><code>=</code><em>value</em>,
where <em>name</em> is the name of the edge and <em>value</em> is the label of the edge
if the edge has a label;</li>
<li><code>format</code>: targets an individual token, the body is a formatting property
for that token,
all tokens in note elements get a <code>format</code> annotation with body <code>note</code>;</li>
<li><code>anno</code>: targets an arbitrary annotation or text range,
body has an arbitrary value;
can be used for extra annotations,
e.g. in the Mondriaan corpus to provide an URL to an artwork derived
from an <code>&lt;rs&gt;</code> element.</li>
</ul>
</li>
<li>
<p><code>namespace</code>: the namespace of the annotation; an indicator where the
information comes from. Possible values:</p>
<ul>
<li><code>pagexml</code>: annotation comes from the PageXML, possibly indirectly, e.g.
<code>h</code>, <code>w</code>, <code>x</code>, <code>y</code></li>
<li><code>tei</code>: annotation comes
<a href="https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_LIT">literally</a>
from the TEI guidelines or the PageXML specification, or is
<a href="https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_LITP">processed</a>
straightforwardly from it;</li>
<li><code><a title="tf" href="../index.html">tf</a></code>: annotation is
<a href="https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_LITC">composed</a>
in a more intricate way from the original source or even
<a href="https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_PROV">added</a>
to it;</li>
<li><code>nlp</code>: annotation is generated as a result of
<a href="https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_NLP">NLP processing</a>;</li>
<li><code>tt</code>: annotation is derived from other material in the source for the benefit
of the Team Text infrastructure. Defined in the <code>watm.yaml</code> file next
to this program.
Currently used for annotations that derive from project specific
requirements.</li>
</ul>
</li>
<li>
<p><code>body</code>: the body of an annotation (probably the <em>kind</em> and <em>body</em> fields
together will make up the body of the resulting web annotation);</p>
</li>
<li>
<p><code>target</code>: a string specifying the target of the annotation, of the
following kinds:</p>
<ul>
<li>
<p><strong>single</strong> this is a target pointing to a single thing, either:</p>
<ul>
<li>
<p><code>fn:bbb-eee</code> a range of text segments in the <code>_ordered_segments</code>
in the file <code>text-fn.json</code>;</p>
<p><strong>N.B.</strong>: we will not have targets that span accross more than one
<code>text-i.json</code> file;
*
an annotation id</p>
</li>
</ul>
</li>
<li>
<p><strong>double</strong> this is a target pointing to two things:</p>
<ul>
<li>
<p><code>fff-&gt;ttt</code> where <code>fff</code> is a "from" target and <code>ttt</code> is a "to" target;
both targets can vary independently between a range and an annotation id.</p>
<p><strong>N,B.</strong> It is allowed that <code>fff</code> and <code>ttt</code> target segments in distinct
<code>text-i.json</code> files.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="caveat">Caveat</h1>
<p>The WATM representation of the corpus is a faithful and complete representation
of the TF dataset and hence of the TEI/PageXML source from which the TF dataset has been
converted.</p>
<p>Well, don't take this too literally, probably there are aspects where the
different representations differ.</p>
<p>I am aware of the following:</p>
<ul>
<li>
<p>The TEI to TF conversion has lost the exact embedding of elements in the
following case:</p>
<p>Suppose element A contains the same words as element B. Then the TF data
does not know whether A is a child of B or the other way round.</p>
<p>This is repairable by adding parenthood edges between nodes when
constructing the TF data. We should then also convert these TF edges to
WATM annotations, for which we need structured targets:</p>
<p>If <code>n</code> is the parent of <code>m</code>, we must make an annotation with body
<code>"parent"</code> and target <code>[n, m]</code>.</p>
<p>Something similar holds for the sibling relationship: if two nodes are adjacent
in a TF dataset, we do not know whether they are siblings elements in the
original XML. It is also possible to add sibling edges to the TF dataset.</p>
<p>See <code><a title="tf.convert.tei" href="tei.html">tf.convert.tei</a></code> under <strong>parentEdges</strong> and <strong>siblingEdges</strong>.</p>
</li>
<li>
<p>The TF to WATM conversion forgets the types of feature values: it does not
make a distinction between the integer <code>1</code> and the string <code>"1"</code>.</p>
<p>This is repairable by creating annotations with structured bodies like
<code>{"att": value}</code> instead of strings like <code>att=value</code> as we do now.</p>
<p>In practice, the meaning of the features in TF are known, and hence the attributes
in the WATM data, so this is not a blocking problem for now.</p>
</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L1-L1597" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;Export to Web Annotation Text Model

# The general idea

This module can export a TF corpus to WATM (Web Annotation Text Model),
which is the input format of the suite of systems developed by Team Text for
serving text plus annotations over the web.

The idea of WATM is, like the idea of Text-Fabric, to untangle the text from its
markup. Everything outside the text itself is coded in annotations.

Annotations look a lot like TF features, but they are a bit more general.
Annotations can also annotate annotations, not only pieces of text.

We need this extra generality, because unlike TF, WATM does not have a concept
of node. The only parallel are the slot nodes of TF, which corresponds to the
tokens of the text in WATM.

Every node in TF is linked to a set of slot nodes.
As such it can be mapped to an annotation to the corresponding tokens.
Features of such nodes can be mapped to annotations to annotations.

TF also has edges. These can be mapped to WATM annotations whose targets are
pairs: one for the thing the edge is *from*, and one for the thing the edge is *to*.
These things are typical annotations that correspond to TF nodes, since TF edges
are links between TF nodes.

If the TF dataset itself is the result of converting an XML file (e.g TEI or
PageXML), then there is a further correspondence between the XML and the TF:

*   elements translate into nodes; element tags translate into node types;
*   attributes translate into features; values of attributes translate into
    values of features.

In our terminology below we assume that the TF data comes from XML files,
but this is not essential. Whenever we talk about *elements* and *tags*,
you may read *nodes* and *node types* if the TF dataset does not have an XML
precursor. Likewise, for *attributes* you may read *features*.

# The specifics

We generate tokens and annotations out of a TF dataset. Here is what we deliver
and in
what form:

*   a bunch of files `text-0.json`, `text-1.json`: with the text segments in an array;
    Each file corresponds with a top-level section in the TF dataset;
*   a bunch of files `anno-1.json`, `anno-2.json, ...: all generated annotations;
    We pack at most 400,000 annotations in one file, that keeps their size
    below 50MB,
    so that they still can live in a git directory without large file support.

## Format of the text files

A `text-i.json` is a JSON file with the following structure:

```
{
  &#34;_ordered_segments&#34;: [
    &#34;token1 &#34;,
    &#34;token2 &#34;,
    ...
  ]
}
```
*   each item in `_ordered_segments` corresponds to one token;
*   the item contains the text of the token plus the subsequent whitespace, if any;
*   if the corpus is converted from TEI, we skip all material inside the
    TEI-header.

### Tokens

Tokens correspond to the slot nodes in the TF dataset.
Depending on the original format of the corpus we have the following specifics.

#### TEI corpora

The base type is `t`, the *atomic* token.
Atomic tokens are tokens as they come from some NLP processing, except when tokens
contain element boundaries. In those cases tokens are split in fragments
between the element boundaries.

It is guaranteed that a text segment that corresponds to a `t` does not contain
element boundaries.

The original, unsplit tokens are also present in the annotations, they have
type `token`.

Tokens have the attributes `str` and `after`, both may be empty.

#### PageXML corpora

The base type is `token`, it is available without NLP processing.

Tokens have the attributes `str` and `after`, both may be empty.
They may also have the attributes `rstr` and `rafter`.

*   `str` is the *logical* string value of a token, `after` is empty or a space:
    what comes after the token before the next token.
*   `rstr` is the raw string value of a token, **when it deviates from the
    logical value**, otherwise no value. `rafter` analogously.

**Example**

token | 1 | 2 | 3 | 4 | 5
--- | --- | --- | --- | --- | ---
rstr | empty | `efflagitan` | `Â¬` | `do` | empty
str | `improbÃ¨` | `efflagitando` | empty | empty | `tandem`

## Format of the annotation files

The `anno-1.json`, `anno-2.json`, ... files are JSON file with the following
structure:

```
{
 &#34;a000nnn&#34;: [
  &#34;kind&#34;,
  &#34;namespace&#34;,
  &#34;body&#34;,
  &#34;target&#34;
 ],{
 ...
}
```

It is a big dictionary, keyed by annotation ids and each value is the data of
an annotation, divided in the following fields:

*   `kind`: the kind of annotation:
    *   `element`: targets the text location where an *element* occurs, the body
        is the element name;
    *   `pi`: targets the text location where a *processing instruction* occurs,
        the body is the  target of the *pi*;
    *   `attribute`: targets an annotation (an *element* or *pi*), the body has
        the shape *name*`=`*value*,
        the name and value of the attribute in question;
    *   `node`: targets an individual *token* or *element* or *pi*,
        the body is the TF node (a number) of that *token* / *element* / *pi*;
    *   `edge`: targets two node annotations, the body has the shape
        `*name* or `*name*`=`*value*,
        where *name* is the name of the edge and *value* is the label of the edge
        if the edge has a label;
    *   `format`: targets an individual token, the body is a formatting property
        for that token,
        all tokens in note elements get a `format` annotation with body `note`;
    *   `anno`: targets an arbitrary annotation or text range,
        body has an arbitrary value;
        can be used for extra annotations,
        e.g. in the Mondriaan corpus to provide an URL to an artwork derived
        from an `&lt;rs&gt;` element.

*   `namespace`: the namespace of the annotation; an indicator where the
    information comes from. Possible values:
    *   `pagexml`: annotation comes from the PageXML, possibly indirectly, e.g.
        `h`, `w`, `x`, `y`
    *   `tei`: annotation comes
        [literally](https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_LIT)
        from the TEI guidelines or the PageXML specification, or is
        [processed](https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_LITP)
        straightforwardly from it;
    *   `tf`: annotation is
        [composed](https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_LITC)
        in a more intricate way from the original source or even
        [added](https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_PROV)
        to it;
    *   `nlp`: annotation is generated as a result of
        [NLP processing](https://annotation.github.io/text-fabric/tf/convert/helpers.html#tf.convert.helpers.CM_NLP);
    *   `tt`: annotation is derived from other material in the source for the benefit
        of the Team Text infrastructure. Defined in the `watm.yaml` file next
        to this program.
        Currently used for annotations that derive from project specific
        requirements.

*   `body`: the body of an annotation (probably the *kind* and *body* fields
    together will make up the body of the resulting web annotation);

*   `target`: a string specifying the target of the annotation, of the
    following kinds:

    *   **single** this is a target pointing to a single thing, either:
        *   `fn:bbb-eee` a range of text segments in the `_ordered_segments`
            in the file `text-fn.json`;

            **N.B.**: we will not have targets that span accross more than one
            `text-i.json` file;
        *   an annotation id

    *   **double** this is a target pointing to two things:
        *   `fff-&gt;ttt` where `fff` is a &#34;from&#34; target and `ttt` is a &#34;to&#34; target;
            both targets can vary independently between a range and an annotation id.

            **N,B.** It is allowed that `fff` and `ttt` target segments in distinct
            `text-i.json` files.

# Caveat

The WATM representation of the corpus is a faithful and complete representation
of the TF dataset and hence of the TEI/PageXML source from which the TF dataset has been
converted.

Well, don&#39;t take this too literally, probably there are aspects where the
different representations differ.

I am aware of the following:

*   The TEI to TF conversion has lost the exact embedding of elements in the
    following case:

    Suppose element A contains the same words as element B. Then the TF data
    does not know whether A is a child of B or the other way round.

    This is repairable by adding parenthood edges between nodes when
    constructing the TF data. We should then also convert these TF edges to
    WATM annotations, for which we need structured targets:

    If `n` is the parent of `m`, we must make an annotation with body
    `&#34;parent&#34;` and target `[n, m]`.

    Something similar holds for the sibling relationship: if two nodes are adjacent
    in a TF dataset, we do not know whether they are siblings elements in the
    original XML. It is also possible to add sibling edges to the TF dataset.

    See `tf.convert.tei` under **parentEdges** and **siblingEdges**.

*   The TF to WATM conversion forgets the types of feature values: it does not
    make a distinction between the integer `1` and the string `&#34;1&#34;`.

    This is repairable by creating annotations with structured bodies like
    `{&#34;att&#34;: value}` instead of strings like `att=value` as we do now.

    In practice, the meaning of the features in TF are known, and hence the attributes
    in the WATM data, so this is not a blocking problem for now.
&#34;&#34;&#34;

import collections
import json
import re

from tf.core.helpers import console
from tf.core.files import initTree, dirContents, expanduser as ex
from tf.core.timestamp import DEEP
from tf.parameters import OTYPE, OSLOTS, URL_TF_DOCS
from tf.app import use


TT_NAME = &#34;watm&#34;

NS_TF = &#34;tf&#34;
NS_PAGEXML = &#34;pagexml&#34;
NS_TEI = &#34;tei&#34;
NS_NLP = &#34;nlp&#34;
NS_TT = &#34;tt&#34;
NS_NONE = &#34;tf&#34;

NS_FROM_OTYPE = dict(
    doc=NS_TF,
    page=NS_TF,
    file=NS_TF,
    folder=NS_TF,
    letter=NS_TF,
    chapter=NS_TF,
    chunk=NS_TF,
    word=NS_TF,
    char=NS_TF,
    token=NS_NLP,
    sentence=NS_NLP,
)
NS_FROM_FEAT = dict(
    otype=NS_TF,
    doc=NS_TF,
    page=NS_TF,
    line=NS_TF,
    after=NS_TF,
    rafter=NS_TF,
    str=NS_TF,
    rstr=NS_TF,
)

KIND_NODE = &#34;node&#34;
KIND_EDGE = &#34;edge&#34;
KIND_ELEM = &#34;element&#34;
KIND_PI = &#34;pi&#34;
KIND_ATTR = &#34;attribute&#34;
KIND_FMT = &#34;format&#34;
KIND_ANNO = &#34;anno&#34;

REL_RE = re.compile(r&#34;&#34;&#34;/tf\b&#34;&#34;&#34;)

TR_SEP_LEVEL = 1


def rep(status):
    &#34;&#34;&#34;Represent a boolean status for a message to the console.

    Parameters
    ----------
    status: boolean

    Returns
    -------
    string
    &#34;&#34;&#34;
    return &#34;OK&#34; if status else &#34;XX&#34;


class WATM:
    &#34;&#34;&#34;The export machinery is exposed as a class, wrapped around a TF dataset.&#34;&#34;&#34;

    def __init__(self, app, nsOrig, skipMeta=False, extra={}):
        &#34;&#34;&#34;Wrap the WATM exporter around a TF dataset.

        Given an already loaded TF dataset, we make an inventory of all data
        we need to perform an export to WATM.

        Parameters
        ----------
        app: object
            A loaded TF dataset, as obtained by a call `use(...)`.
            See `tf.app.use`
        nsOrig: string
            A namespace corresponding to the format of the original, pre-Text-Fabric
            representation. For example `tei` for a TEI corpus, `pagexml` for a
            PageXML corpus. The namespace is not related to XML namespaces, it is
            merely a device to categorize the resulting annotations.
        skipMeta: boolean, optional False
            Only relevant for TEI corpora. If True, all material in the TEI Header
            will not be converted to tokens in the text.
            More precisely: all TF slots for which the feature `is_meta` has a true-ish
            value will be skipped. If there is no feature `is_meta` in the dataset,
            the setting of `skipMeta` will have no effect: nothing will be excluded.
        extra: dictionary, optional {}
            The data for extra annotations, which will be generated on the fly under the
            namespace `anno`. The keys are the names of features/attributes, the
            value for each key is a dictionary that maps nodes to values.
        &#34;&#34;&#34;
        self.app = app
        self.nsOrig = nsOrig
        self.extra = extra
        api = app.api
        F = api.F
        E = api.E
        T = api.T
        sectionTypes = T.sectionTypes

        if len(sectionTypes) == 0:
            console(
                &#34;No section types in corpus. &#34;
                &#34;We need at least one section level for tier-0&#34;,
                error=True,
            )
            self.error = True
        else:
            tierType = T.sectionTypes[0]
            console(f&#34;Tier 0 is section level &#39;{tierType}&#39;&#34;)
            self.tierType = tierType
            self.error = False

        self.L = api.L
        self.Es = api.Es
        self.F = F
        self.E = E
        self.Fs = api.Fs
        self.slotType = self.F.otype.slotType
        self.otypes = self.F.otype.all
        self.info = app.info
        self.repoLocation = app.repoLocation

        Fall = api.Fall
        Eall = api.Eall
        self.Fall = Fall
        self.Eall = Eall

        excludedFeatures = {OTYPE, OSLOTS, &#34;after&#34;, &#34;str&#34;}
        self.nodeFeatures = [f for f in Fall() if f not in excludedFeatures]
        self.edgeFeatures = [f for f in Eall() if f not in excludedFeatures]

        FAllSet = set(Fall())

        self.fotypev = F.otype.v
        self.eoslots = E.oslots.s
        self.emptyv = F.empty.v if &#34;empty&#34; in FAllSet else None
        self.strv = F.str.v if &#34;str&#34; in FAllSet else None
        self.rstrv = F.rstr.v if &#34;rstr&#34; in FAllSet else None
        self.afterv = F.after.v if &#34;after&#34; in FAllSet else None
        self.rafterv = F.rafter.v if &#34;rafter&#34; in FAllSet else None
        is_metav = F.is_meta.v if &#34;is_meta&#34; in FAllSet else None
        self.is_metav = is_metav

        app.dm(f&#34;[WATM exporter docs]({URL_TF_DOCS}/convert/watm.html)&#34;)

        if skipMeta and not is_metav:
            console(
                &#34;skipMeta=True has no effect because feature is_meta is not defined.&#34;,
                error=True,
            )
            skipMeta = False

        self.skipMeta = skipMeta

    def makeText(self):
        &#34;&#34;&#34;Creates the text data.

        The text is a list of tokens and will be stored in member `text` in this object.
        Additionally, the mapping from slot numbers in the TF data
        to indices in this list is stored in member `tlFromTf`.
        &#34;&#34;&#34;
        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        F = self.F
        L = self.L
        slotType = self.slotType
        tierType = self.tierType
        skipMeta = self.skipMeta

        emptyv = self.emptyv
        strv = self.strv
        rstrv = self.rstrv
        afterv = self.afterv
        rafterv = self.rafterv
        is_metav = self.is_metav

        texts = []
        tlFromTf = {}

        self.texts = texts
        self.tlFromTf = tlFromTf

        for ti, sec0 in enumerate(F.otype.s(tierType)):
            text = []
            texts.append(text)

            for s in L.d(sec0, otype=slotType):
                if skipMeta and is_metav(s):
                    continue

                after = rafterv(s) if rafterv else None

                if after is None:
                    after = afterv(s) if afterv else None

                if after is None:
                    after = &#34;&#34;

                if emptyv and emptyv(s):
                    value = after
                else:
                    string = rstrv(s) if rstrv else None

                    if string is None:
                        string = strv(s) if strv else None

                    if string is None:
                        string = &#34;&#34;

                    value = f&#34;{string}{after}&#34;

                text.append(value)
                t = len(text) - 1
                tlFromTf[s] = (ti, t)

    def mkAnno(self, kind, ns, body, target):
        &#34;&#34;&#34;Make a single annotation and return its id.

        Parameters
        ----------
        kind: string
            The kind of annotation.
        ns: string
            The namespace of the annotation.
        body: string
            The body of the annotation.
        target: string  or tuple of strings
            The target of the annotation.
        &#34;&#34;&#34;
        annos = self.annos
        aId = f&#34;a{len(annos):&gt;08}&#34;
        annos.append((kind, aId, ns, body, target))
        return aId

    def makeAnno(self):
        &#34;&#34;&#34;Make all annotations.

        The annotations are stored in a big list, in member `anno` of this object.

        The mapping from slots to indices in the list of tokens is now extended
        with the mapping from nodes to corresponding node annotations.

        So member `tlFromTf` is now a full mapping from all nodes in TF to
        tokens and/or annotations in WATM.
        &#34;&#34;&#34;
        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        Es = self.Es
        F = self.F
        Fs = self.Fs
        fotypev = self.fotypev
        eoslots = self.eoslots
        nodeFeatures = self.nodeFeatures
        edgeFeatures = self.edgeFeatures
        slotType = self.slotType
        otypes = self.otypes
        nsOrig = self.nsOrig
        skipMeta = self.skipMeta
        extra = self.extra

        tlFromTf = self.tlFromTf

        is_metav = self.is_metav

        isTei = nsOrig == NS_TEI

        annos = []
        texts = self.texts
        self.annos = annos

        invertedTargets = []
        farTargets = []

        def mkTarget(n):
            ts = tlFromTf[n]
            return f&#34;{ts[0]}:{ts[1]}-{ts[1] + 1}&#34; if fotypev(n) == slotType else ts

        for otype in otypes:
            isSlot = otype == slotType

            for n in F.otype.s(otype):
                if isSlot:
                    if skipMeta and is_metav(n):
                        continue

                    self.mkAnno(KIND_NODE, NS_TF, n, mkTarget(n))
                else:
                    ws = eoslots(n)
                    if skipMeta and (is_metav(ws[0]) or is_metav(ws[-1])):
                        continue

                    ti0, start = tlFromTf[ws[0]]
                    ti1, end = tlFromTf[ws[-1]]

                    if ti0 != ti1:
                        farTargets.append((otype, ti0, start, ti1, end))
                        continue

                    if end &lt; start:
                        invertedTargets.append((otype, ti0, start, end))
                        start, end = (end, start)

                    target = f&#34;{ti0}:{start}-{end + 1}&#34;
                    aId = (
                        self.mkAnno(KIND_PI, nsOrig, otype[1:], target)
                        if otype.startswith(&#34;?&#34;)
                        else self.mkAnno(
                            KIND_ELEM, NS_FROM_OTYPE.get(otype, nsOrig), otype, target
                        )
                    )
                    tlFromTf[n] = aId
                    self.mkAnno(KIND_NODE, NS_TF, n, aId)

        for feat in nodeFeatures:
            ns = Fs(feat).meta.get(&#34;conversionCode&#34;, NS_FROM_FEAT.get(feat, nsOrig))

            if ns is None:
                console(
                    f&#34;Node feature {feat} has no namespace, &#34;
                    f&#34;defaulting to {NS_NONE}&#34;,
                    error=True,
                )
                ns = NS_NONE

            isRend = False
            isNote = False

            if isTei:
                parts = feat.split(&#34;_&#34;, 2)
                isRend = len(parts) &gt;= 2 and parts[0] == &#34;rend&#34;
                isNote = len(parts) == 2 and parts[0] == &#34;is&#34; and parts[1] == &#34;note&#34;

            if isRend or isNote:
                body = parts[1] if isRend else &#34;note&#34;

                for n, val in Fs(feat).items():
                    if not val or (skipMeta and is_metav(n)):
                        continue

                    self.mkAnno(KIND_FMT, ns, body, mkTarget(n))
            else:
                for n, val in Fs(feat).items():
                    if val is None or skipMeta and is_metav(n):
                        continue

                    body = f&#34;{feat}={val}&#34;
                    self.mkAnno(KIND_ATTR, ns, body, mkTarget(n))

        for feat in edgeFeatures:
            ns = Es(feat).meta.get(&#34;conversionCode&#34;, NS_FROM_FEAT.get(feat, nsOrig))

            if ns is None:
                console(
                    f&#34;Edge feature {feat} has no conversion code, &#34;
                    f&#34;defaulting to {NS_NONE}&#34;,
                    error=True,
                )
                ns = NS_NONE

            for fromNode, toNodes in Es(feat).items():
                if skipMeta and is_metav(fromNode):
                    continue

                if fromNode not in tlFromTf:
                    continue

                targetFrom = mkTarget(fromNode)

                if type(toNodes) is dict:
                    for toNode, val in toNodes.items():
                        if skipMeta and is_metav(toNode):
                            continue

                        if toNode not in tlFromTf:
                            continue

                        body = f&#34;{feat}={val}&#34;
                        targetTo = mkTarget(toNode)
                        target = f&#34;{targetFrom}-&gt;{targetTo}&#34;
                        self.mkAnno(KIND_EDGE, ns, body, target)
                else:
                    for toNode in toNodes:
                        if skipMeta and is_metav(toNode):
                            continue

                        if toNode not in tlFromTf:
                            continue

                        targetTo = mkTarget(toNode)
                        target = f&#34;{targetFrom}-&gt;{targetTo}&#34;
                        self.mkAnno(KIND_EDGE, ns, feat, target)

        for feat, featData in extra.items():
            for n, value in featData.items():
                self.mkAnno(KIND_ANNO, NS_TT, f&#34;{feat}={value}&#34;, mkTarget(n))

        if len(invertedTargets):
            console(f&#34;WARNING: inverted targets, {len(invertedTargets)}x&#34;)
            for otype, ti0, start, end in invertedTargets:
                text = texts[ti0]
                sega = text[start]
                segb = text[end - 1]
                console(f&#34;{otype:&gt;20} {start:&gt;6} `{sega}` &gt; {end - 1} `{segb}`&#34;)

        if len(farTargets):
            console(
                f&#34;ERROR: targets across tier0 items, {len(farTargets)}x&#34;,
                error=True,
            )
            for otype, ti0, start, ti1, end in farTargets:
                sega = texts[ti0][start]
                segb = texts[ti1][end - 1]
                console(
                    f&#34;{otype:&gt;20} {ti0:&gt;2}:{start:&gt;6} `{sega}` - &#34;
                    f&#34;{ti1:&gt;2}:{end - 1} `{segb}`&#34;
                )

    def writeAll(self):
        &#34;&#34;&#34;Write text and annotation data to disk.

        The data will be written as JSON files.
        When the annotation data grows larger than a certain threshold, it will be
        divided over several files.

        The annotations are sorted by annotation id.
        &#34;&#34;&#34;

        # text files

        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        app = self.app
        texts = self.texts
        annos = self.annos

        baseDir = self.repoLocation
        relative = app.context.relative
        version = app.version
        wRelative = REL_RE.sub(f&#34;/{TT_NAME}/{version}/&#34;, relative, count=1)
        resultDir = f&#34;{baseDir}{wRelative}&#34;

        textFiles = []
        self.textFiles = textFiles

        initTree(resultDir, fresh=True)

        total = 0

        for i, text in enumerate(texts):
            textFile = f&#34;{resultDir}/text-{i}.json&#34;
            textFiles.append(textFile)
            nText = len(text)
            total += nText

            with open(textFile, &#34;w&#34;) as fh:
                json.dump(
                    dict(_ordered_segments=text), fh, ensure_ascii=False, indent=1
                )

            console(f&#34;Text file {i:&gt;4}: {nText:&gt;8} segments to {textFile}&#34;)

        nTextFiles = len(textFiles)
        sep = &#34;&#34; if nTextFiles == 1 else &#34;s&#34;
        console(f&#34;Text files all: {total:&gt;8} segments to {nTextFiles} file{sep}&#34;)

        # annotation files

        annoStore = {}

        for kind, aId, ns, body, target in annos:
            annoStore[aId] = (kind, ns, body, target)

        aIdSorted = sorted(annoStore.keys())

        annoFile = f&#34;{resultDir}/anno.tsv&#34;

        if False:
            with open(annoFile, &#34;w&#34;) as fh:
                for aId in aIdSorted:
                    kind, ns, body, target = annoStore[aId]
                    fh.write(f&#34;{aId}\t{kind}\t{ns}\t{body}\t{target}\n&#34;)

        thisAnnoStore = {}
        thisA = 1
        annoFiles = []
        self.annoFiles = annoFiles

        LIMIT = 400000
        j = 0
        total = 0

        def writeThis():
            annoFile = f&#34;{resultDir}/anno-{thisA:&gt;01}.json&#34;
            annoFiles.append(annoFile)

            with open(annoFile, &#34;w&#34;) as fh:
                json.dump(thisAnnoStore, fh, ensure_ascii=False, indent=1)

            console(f&#34;Anno file {i:&gt;4}: {j:&gt;8} annotations written to {annoFile}&#34;)

        for aId in aIdSorted:
            if j &gt;= LIMIT:
                writeThis()
                thisA += 1
                thisAnnoStore = {}
                total += j
                j = 0

            thisAnnoStore[aId] = annoStore[aId]
            j += 1

        if len(thisAnnoStore):
            writeThis()
            total += j

        if len(annos) != total:
            console(f&#34;Sum of batches : {total:&gt;8}&#34;)
            console(f&#34;All annotations: {len(annoStore):&gt;8}&#34;)
            console(&#34;Mismatch in number of annotations&#34;, error=True)

        nAnnoFiles = len(annoFiles)
        sep = &#34;&#34; if nAnnoFiles == 1 else &#34;s&#34;
        console(f&#34;Anno files all: {total:&gt;8} annotations to {nAnnoFiles} file{sep}&#34;)

    @staticmethod
    def compare(nTF, nWA):
        &#34;&#34;&#34;Compare two numbers and report the outcome.

        Used for testing the WATM conversion.

        Parameters
        ----------
        nTF: integer
            The number as it is counted from the original TF dataset.
        nWA: integer
            The number as it is counted from the generated WATM dataset.

        Returns
        -------
        boolean
            Whether the two values are equal.
        &#34;&#34;&#34;
        console(f&#34;\tTF: {nTF:&gt;6}\n\tWA: {nWA:&gt;6}&#34;, error=nTF != nWA)
        return nTF == nWA

    @staticmethod
    def strEqual(wa=None, tf=None):
        &#34;&#34;&#34;Compare two strings and report the outcome.

        Used for testing the WATM conversion.

        Parameters
        ----------
        nTF: string
            The string as encountered in the original TF dataset.
        nWA: string
            The string as encountered in the generated WATM dataset.

        Returns
        -------
        boolean
            Whether the two values are equal.
        &#34;&#34;&#34;
        different = False

        for i, cTF in enumerate(tf):
            if i &gt;= len(wa):
                contextI = max((0, i - 10))
                console(f&#34;\tWA {i}: {wa[contextI:i]} &lt;END&gt;&#34;, error=True)
                console(f&#34;\tTF {i}: {tf[contextI:i]} &lt;&gt; {tf[i:i + 10]}&#34;, error=True)
                different = True
                break
            elif tf[i] != wa[i]:
                contextI = max((0, i - 10))
                console(
                    f&#34;\tWA {i}: {wa[contextI:i]} &lt;{wa[i]}&gt; {wa[i + 1:i + 11]}&#34;,
                    error=True,
                )
                console(
                    f&#34;\tTF {i}: {tf[contextI:i]} &lt;{tf[i]}&gt; {tf[i + 1:i + 11]}&#34;,
                    error=True,
                )
                different = True
                break

        if not different and len(wa) &gt; len(tf):
            i = len(tf)
            contextI = max((0, i - 10))
            console(f&#34;\tWA {i}: {wa[contextI:i]} &lt;&gt; {wa[i:i + 10]}&#34;, error=True)
            console(f&#34;\tTF {i}: {tf[contextI:i]} &lt;END&gt;&#34;, error=True)
            different = True

        sampleWA = f&#34;{wa[0:20]} ... {wa[-20:]}&#34;.replace(&#34;\n&#34;, &#34; &#34;)
        sampleTF = f&#34;{tf[0:20]} ... {tf[-20:]}&#34;.replace(&#34;\n&#34;, &#34; &#34;)
        console(f&#34;\tTF: {sampleTF:&gt;6}\n\tWA: {sampleWA:&gt;6}&#34;)
        return not different

    def testAll(self):
        &#34;&#34;&#34;Test all aspects of the WATM conversion.

        For all kinds of information, such as nodes, edges, features, tokens,
        annotations, we check whether the parts that should correspond between
        the TF dataset and the WATM annotations do so indeed.

        We present some statistics, and highlight the mismatches.

        Returns
        -------
        boolean
            Whether all things that must agree do indeed agree.
        &#34;&#34;&#34;
        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        self.testSetup()

        good = True

        if not self.testText():
            good = False

        if not self.testElements():
            good = False

        if not self.testAttributes():
            good = False

        if not self.testExtra():
            good = False

        if not self.testEdges():
            good = False

        console(&#34;Overall outcome ...&#34;)
        console(f&#34;{rep(good)} - whether all tests passed&#34;, error=not good)

        return good

    def testSetup(self):
        &#34;&#34;&#34;Prepare the tests.

        We read the WATM dataset and store the tokens in member `testTokens`
        and the annotations in the member `testAnnotations`.
        We unpack targets if they contain structured information.
        &#34;&#34;&#34;
        textFiles = self.textFiles
        annoFiles = self.annoFiles

        tokenFiles = []

        for textFile in textFiles:
            with open(textFile) as fh:
                text = json.load(fh)
                tokens = text[&#34;_ordered_segments&#34;]
                tokenFiles.append(tokens)

        self.testTokens = tokenFiles

        annotations = []

        for annoFile in annoFiles:
            with open(annoFile) as fh:
                annos = json.load(fh)

                for aId, (kind, ns, body, target) in annos.items():
                    if &#34;-&gt;&#34; in target:
                        parts = target.split(&#34;-&gt;&#34;, 1)
                    else:
                        parts = [target]

                    newParts = []

                    for part in parts:
                        if &#34;-&#34; in part:
                            file, part = part.split(&#34;:&#34;, 1)
                            start, end = part.split(&#34;-&#34;, 1)
                            part = (int(file), int(start), int(end))

                        newParts.append(part)

                    target = newParts[0] if len(newParts) == 1 else tuple(newParts)

                    annotations.append((aId, kind, body, target))

        annotations = sorted(annotations)
        self.testAnnotations = annotations

    def testText(self):
        &#34;&#34;&#34;Test the text.

        We test the number of tokens and the equality of the resulting text:
        whether the TF and WATM datasets agree on it.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        F = self.F
        skipMeta = self.skipMeta
        is_metav = self.is_metav
        tokenFiles = self.testTokens
        texts = self.texts

        console(&#34;Testing the text ...&#34;)

        nTokensTF = sum(
            0 if skipMeta and is_metav(s) else 1 for s in range(1, F.otype.maxSlot + 1)
        )
        nTokensWA = sum(len(tokens) for tokens in tokenFiles)
        nGood = self.compare(nTokensTF, nTokensWA)
        console(f&#34;{rep(nGood)} - whether the amounts of tokens agree&#34;, error=not nGood)

        textWA = &#34;&#34;.join(&#34;&#34;.join(tokens) for tokens in tokenFiles)
        textTF = &#34;&#34;.join(&#34;&#34;.join(text) for text in texts)

        tGood = self.strEqual(wa=textWA, tf=textTF)
        console(f&#34;{rep(tGood)} - whether the text is the same&#34;, error=not tGood)

        return nGood and tGood

    def testElements(self):
        &#34;&#34;&#34;Test the elements.

        We test the annotations representing elements/processing instructions
        and check whether they correspond 1-1 to the non-slot nodes in the TF
        dataset.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        F = self.F
        fotypev = self.fotypev
        eoslots = self.eoslots
        skipMeta = self.skipMeta
        is_metav = self.is_metav
        annotations = self.testAnnotations

        console(&#34;Testing the elements ...&#34;)

        nElementsTF = 0
        nPisTF = 0

        for n in range(F.otype.maxSlot + 1, F.otype.maxNode + 1):
            nType = fotypev(n)
            isPi = nType.startswith(&#34;?&#34;)

            if isPi:
                nPisTF += 1

            slots = eoslots(n)
            b = slots[0]
            e = slots[-1]

            if skipMeta and (is_metav(b) or is_metav(e)):
                continue
            else:
                if not isPi:
                    nElementsTF += 1

        nElementsWA = sum(1 if a[1] == &#34;element&#34; else 0 for a in annotations)
        nPisWA = sum(1 if a[1] == &#34;pi&#34; else 0 for a in annotations)

        eGood = self.compare(nElementsTF, nElementsWA)
        console(
            f&#34;{rep(eGood)} - whether the amounts of elements and nodes agree&#34;,
            error=not eGood,
        )

        console(&#34;Testing the processing instructions ...&#34;)

        pGood = self.compare(nPisTF, nPisWA)
        console(
            f&#34;{rep(pGood)} - whether the amounts of processing instructions agree&#34;,
            error=not pGood,
        )

        console(&#34;Testing the element annotations ...&#34;)

        tfFromAid = {}

        element = 0
        pi = 0
        other = 0
        good = 0
        wrong = 0
        unmapped = 0

        for aId, kind, body, target in annotations:
            if kind == &#34;node&#34;:
                tfFromAid[target] = body

        self.tfFromAid = tfFromAid

        console(f&#34;\t{len(tfFromAid)} element annotations&#34;)

        for aId, kind, body, target in annotations:
            isElem = kind == &#34;element&#34;
            isPi = kind == &#34;pi&#34;

            if not isElem and not isPi:
                other += 1
                continue

            if isElem:
                element += 1
            else:
                pi += 1

            tag = body
            node = tfFromAid.get(aId, None)
            if node is None:
                unmapped += 1
                continue

            otype = fotypev(node)

            if isPi and tag == otype[1:] or not isPi and tag == otype:
                good += 1
            else:
                wrong += 1

        console(f&#34;\tElement : {element:&gt;5} x&#34;)
        console(f&#34;\tPi      : {pi:&gt;5} x&#34;)
        console(f&#34;\tOther   : {other:&gt;5} x&#34;)
        console(f&#34;\tGood    : {good:&gt;5} x&#34;)
        console(f&#34;\tWrong   : {wrong:&gt;5} x&#34;)
        console(f&#34;\tUnmapped: {unmapped:&gt;5} x&#34;)

        aGood = wrong == 0 and unmapped == 0
        console(
            f&#34;{rep(aGood)} - whether all element annotations are ok&#34;, error=not aGood
        )

        return aGood and eGood and pGood

    def testAttributes(self):
        &#34;&#34;&#34;Test the attributes.

        We test whether attributes and features correspond to each other.

        Some attributes in the original TEI are converted in a special way into
        TF features: this holds for the `rend` attribute.
        Basically, a value `rend=&#34;italic&#34;` is translated into feature
        `is_italic=1`.
        In turn, these features have been translated into annotations of kind
        `format`. We test them separately.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        Fs = self.Fs
        Fall = self.Fall
        eoslots = self.eoslots
        skipMeta = self.skipMeta
        is_metav = self.is_metav
        annotations = self.testAnnotations
        tfFromAid = self.tfFromAid
        nsOrig = self.nsOrig

        isTei = nsOrig == NS_TEI

        console(&#34;Testing the attributes ...&#34;)

        attWA = []

        for aId, kind, body, target in annotations:
            if kind != &#34;attribute&#34;:
                continue
            node = tfFromAid[target]
            att, value = body.split(&#34;=&#34;, 1)
            attWA.append((node, att, value))

        attWA = sorted(attWA)

        console(f&#34;\t{len(attWA)} attribute values&#34;)

        good = 0
        wrong = []

        for node, att, valWA in attWA:
            valTF = str(Fs(att).v(node))
            if valWA == valTF:
                good += 1
            else:
                wrong.append((node, att, valWA, valTF))

        console(f&#34;\tGood:     {good:&gt;5} x&#34;)
        console(f&#34;\tWrong:    {len(wrong):&gt;5} x&#34;)
        consistent = len(wrong) == 0

        console(
            f&#34;{rep(consistent)} - whether annotations are consistent with features&#34;,
            error=not consistent,
        )

        attTF = []

        for feat in Fall():
            if feat in {&#34;otype&#34;, &#34;str&#34;, &#34;after&#34;}:
                continue

            if skipMeta and feat == &#34;is_meta&#34;:
                continue

            if isTei and (
                (feat != &#34;is_meta&#34; and feat.startswith(&#34;is_&#34;))
                or feat.startswith(&#34;rend_&#34;)
            ):
                continue

            for node, valTF in Fs(feat).items():
                slots = eoslots(node)
                b = slots[0]
                e = slots[-1]

                if skipMeta and (is_metav(b) or is_metav(e)):
                    continue

                attTF.append((node, feat, str(valTF)))

        attTF = sorted(attTF)

        console(f&#34;\tWA attributes: {len(attWA)}&#34;)
        console(f&#34;\tTF attributes: {len(attTF)}&#34;)
        complete = attTF == attWA
        console(
            f&#34;{rep(complete)} - whether annotations are complete w.r.t. features&#34;,
            error=not complete,
        )

        console(&#34;Testing the format attributes ...&#34;)

        fmtWA = []

        for aId, kind, body, target in annotations:
            if kind != &#34;format&#34;:
                continue
            if body == &#34;note&#34;:
                continue
            node = tfFromAid[target]
            fmtWA.append((node, body))

        fmtWA = sorted(fmtWA)
        fmtWaFreq = collections.Counter()

        for node, body in fmtWA:
            fmtWaFreq[body] += 1

        console(f&#34;\t{len(fmtWA)} format values&#34;)
        console(&#34;\tformatting attributes: &#34;)
        for fa, n in sorted(fmtWaFreq.items(), key=lambda x: (-x[1], x[0])):
            console(f&#34;\t\t{n:&gt;6} x {fa}&#34;)

        good = 0
        wrong = []

        for node, valWA in fmtWA:
            feat = f&#34;rend_{valWA}&#34;
            valTF = valWA if str(Fs(feat).v(node)) else None
            if valWA == valTF:
                good += 1
            else:
                wrong.append((node, feat, valWA, valTF))

        console(f&#34;\tGood:     {good:&gt;5} x&#34;)
        console(f&#34;\tWrong:    {len(wrong):&gt;5} x&#34;)
        fconsistent = len(wrong) == 0
        console(
            f&#34;{rep(fconsistent)} - &#34;
            f&#34;whether format annotationsare consistent with features&#34;,
            error=not fconsistent,
        )

        fmtTF = []

        for feat in Fall():
            if not feat.startswith(&#34;rend_&#34;):
                continue

            value = feat.split(&#34;_&#34;, 2)[1]
            if value == &#34;note&#34;:
                continue

            for node, valTF in Fs(feat).items():
                slots = eoslots(node)
                b = slots[0]
                e = slots[-1]

                if skipMeta and (is_metav(b) or is_metav(e)):
                    continue

                fmtTF.append((node, value))

        fmtTF = sorted(fmtTF)

        console(f&#34;\tWA format attributes: {len(fmtWA)}&#34;)
        console(f&#34;\tTF format attributes: {len(fmtTF)}&#34;)
        fcomplete = fmtTF == fmtWA
        console(
            f&#34;{rep(complete)} - &#34;
            f&#34;whether format annotations are complete w.r.t. features&#34;,
            error=not fcomplete,
        )

        return consistent and complete and fconsistent and fcomplete

    def testExtra(self):
        &#34;&#34;&#34;Test the extra data for on-the-fly annotations.

        Annotations that have been generated out of the data stored in the
        `extra` parameter with which the object has been initialized, all got
        the kind `anno`.

        Now we check these annotations against the data that went into it.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        annotations = self.testAnnotations
        tfFromAid = self.tfFromAid
        extra = self.extra

        console(&#34;Testing the extra annotations ...&#34;)

        attWA = []

        for aId, kind, body, target in annotations:
            if kind != &#34;anno&#34;:
                continue
            node = tfFromAid[target]
            att, value = body.split(&#34;=&#34;, 1)
            attWA.append((node, att, value))

        attWA = sorted(attWA)

        attEX = []

        for feat, featData in extra.items():
            for n, value in featData.items():
                attEX.append((n, feat, value))

        attEX = sorted(attEX)

        console(f&#34;\t{len(attEX)} extra feature values&#34;)
        console(f&#34;\t{len(attWA)} extra annotations&#34;)

        good = attWA == attEX

        def showData(tuples, isin, isout):
            data = {}

            for n, f, v in tuples:
                data.setdefault(f, {})[n] = v

            for f in sorted(data):
                fData = data[f]
                console(
                    f&#34;\t{isin}: {f} misses {len(fData)} annotations in {isout}&#34;,
                    error=True,
                )
                for n in sorted(fData.keys())[0:3]:
                    console(f&#34;\t\t\t{n:&gt;7} = {fData[n]}&#34;, error=True)

        if not good:
            attWASet = set(attWA)
            attEXSet = set(attEX)

            onlyWA = attWASet - attEXSet
            onlyEX = attEXSet - attWASet

            if len(onlyWA):
                showData(onlyWA, &#34;WA&#34;, &#34;EX&#34;)
            else:
                console(&#34;\tWA: All extra annotations derive from the extra data&#34;)
            if len(onlyEX):
                showData(onlyEX, &#34;EX&#34;, &#34;WA&#34;)
            else:
                console(&#34;\tEX: All extra data ended up as annotations&#34;)

        console(f&#34;{rep(good)} - whether the extra annotations agree&#34;, error=not good)

        return good

    def testEdges(self):
        &#34;&#34;&#34;Test the edges.

        Edges in TF are links between nodes, and they translate into annotations of
        kind `edge` which target a pair of annotations: the `from` annotation,
        and the `to` annotation.

        Here we check whether the TF edges are faithfully and completely parallelled
        by annotations.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        Es = self.Es
        Eall = self.Eall
        annotations = self.testAnnotations

        console(&#34;Testing the edges ...&#34;)

        tfFromWANodes = {}
        tfFromWAEdges = {}

        for aId, kind, body, target in annotations:
            if kind != &#34;node&#34;:
                continue
            if type(target) is tuple:
                file, start, end = target
                if start + 1 != end:
                    # we expect that node annotations either targets a single token
                    # or an element/pi annotation
                    print(target)
                    break
                target = (file, end)
            tfFromWANodes[target] = body

        for aId, kind, body, target in annotations:
            if kind != &#34;edge&#34;:
                continue

            fro, to = target
            fromNode = tfFromWANodes[fro]
            toNode = tfFromWANodes[to]
            parts = body.split(&#34;=&#34;, 1)
            name, val = (body, None) if len(parts) == 1 else parts
            tfFromWAEdges.setdefault(name, {}).setdefault(fromNode, {})[toNode] = val

        console(f&#34;\tFound: {len(tfFromWANodes)} nodes&#34;)

        for edge, edgeData in sorted(tfFromWAEdges.items()):
            console(f&#34;\tFound edge {edge} with {len(edgeData)} starting nodes&#34;)

        allGood = True

        for edge in set(Eall()) | set(tfFromWAEdges):
            if edge == &#34;oslots&#34;:
                continue

            console(f&#34;\tChecking edge {edge}&#34;)

            good = True

            if edge not in set(Eall()):
                console(&#34;\t\tmissing in TF data&#34;, error=True)
                good = False

            if edge not in tfFromWAEdges:
                console(&#34;\t\tmissing in annotation data&#34;, error=True)
                good = False

            if not good:
                continue

            dataTF = dict(Es(edge).items())
            dataWA = tfFromWAEdges[edge]

            fromNodesTF = set(dataTF)
            fromNodesWA = set(dataWA)

            nFromTF = len(fromNodesTF)
            nFromWA = len(fromNodesWA)

            if fromNodesTF == fromNodesWA:
                console(f&#34;\t\tsame {nFromTF} fromNodes&#34;)
            else:
                console(
                    f&#34;\t\tfrom nodes differ: {nFromTF} in TF, {nFromWA} in WA&#34;,
                    error=True,
                )
                good = False

            diffs = []

            nToChecked = 0

            for f, toNodeInfoTF in dataTF.items():
                toNodeInfoWA = dataWA[f]
                if type(toNodeInfoTF) is dict:
                    toNodeInfoTF = {k: str(v) for (k, v) in toNodeInfoTF.items()}
                else:
                    toNodeInfoTF = {x: None for x in toNodeInfoTF}

                if toNodeInfoTF != toNodeInfoWA:
                    diffs.append((f, toNodeInfoTF, toNodeInfoWA))

                nToChecked += len(toNodeInfoTF)

            if len(diffs):
                good = False
                console(
                    f&#34;\t\tdifferences in toNodes for {len(diffs)} fromNodes&#34;, error=True
                )

                for f, toNodeInfoTF, toNodeInfoWA in sorted(diffs)[0:10]:
                    console(f&#34;\t\t\tfromNode {f}&#34;, error=True)

                    toNodesTF = set(toNodeInfoTF)
                    toNodesWA = set(toNodeInfoWA)

                    nToTF = len(toNodesTF)
                    nToWA = len(toNodesWA)

                    if toNodesTF == toNodesWA:
                        console(f&#34;\t\t\tsame {nToTF} toNodes&#34;)
                    else:
                        console(
                            f&#34;\t\t\ttoNodes differ: {nToTF} in TF, {nToWA} in WA&#34;,
                            error=True,
                        )
                    for t in toNodesTF | toNodesWA:
                        doCompare = True
                        if t not in toNodesTF:
                            console(f&#34;\t\t\t\ttoNode {t} not in TF&#34;, error=True)
                            doCompare = False
                        else:
                            valTF = toNodeInfoTF[t]

                        if t not in toNodesWA:
                            console(f&#34;\t\t\t\ttoNode {t} not in WA&#34;, error=True)
                            doCompare = False
                        else:
                            valWA = toNodeInfoWA[t]

                        if doCompare:
                            if valTF == valWA:
                                console(
                                    f&#34;\t\t\t\ttoNode{t} values agree: {repr(valTF)}&#34;
                                )
                            else:
                                console(
                                    f&#34;\t\t\t\ttoNode{t} values differ: &#34;
                                    f&#34;TF: {repr(valTF)} WA: {repr(valWA)}&#34;,
                                    error=True,
                                )

            console(f&#34;\t{rep(good)} - {nToChecked} toNodes checked&#34;, error=not good)

            if not good:
                allGood = False

        console(f&#34;{rep(allGood)} - whether all edges agree&#34;)

        return allGood


class WATMS:
    &#34;&#34;&#34;Export corpora that are divided over multiple TF datasets.

    We set up and run WATM objects for each TF dataset, and generate results
    for them separately.

    We assume that all corpora have been generated by the same method and originate
    from the same original format.

    They must reside in the same repository, in adjacent directories under the `tf`
    top-level directory of the repo.
    &#34;&#34;&#34;

    def __init__(self, org, repo, backend, nsOrig, skipMeta=False, extra={}):
        &#34;&#34;&#34;Collect the parameters for the WATM machinery.

        We will initialize many `WATM` objects with mostly the same parameters.
        These are collected when we initialize this object.

        Parameters
        ----------
        org: string
            The organization of all TF datasets.
        repo: string
            The repo of all TF datasets.
        backend: string
            The backend of all TF datasets.
        nsOrig: string
            The original namespace of all TF datasets.
            See `tf.convert.watm.WATM`.
        skipMeta: boolean, optional False
            See `tf.convert.watm.WATM`.
        extra: dictionary, optional {}
            See `tf.convert.watm.WATM`.
        &#34;&#34;&#34;
        self.org = org
        self.repo = repo
        self.backend = backend
        self.nsOrig = nsOrig
        self.skipMeta = skipMeta
        self.extra = extra

        repoDir = ex(f&#34;~/{backend}/{org}/{repo}&#34;)
        tfDir = f&#34;{repoDir}/tf&#34;
        docs = dirContents(tfDir)[1]
        console(f&#34;Found {len(docs)} docs in {tfDir}&#34;)
        self.docs = docs

    def produce(self, doc=None):
        &#34;&#34;&#34;Convert all relevant TF datasets.

        Parameters
        ----------
        doc: string, optional None
            Subdirectory where one of the TF datasets resides.
            If passed, only this dataset will be converted.
            Otherwise all datasets will be converted.
        &#34;&#34;&#34;
        org = self.org
        repo = self.repo
        backend = self.backend
        nsOrig = self.nsOrig
        skipMeta = self.skipMeta
        extra = self.extra
        docs = self.docs

        chosenDoc = doc

        for doc in sorted(docs, key=lambda x: (x[0], int(x[1:]))):
            if chosenDoc is not None and chosenDoc != doc:
                continue

            console(f&#34;{doc:&gt;5} ... &#34;, newline=False)
            A = use(
                f&#34;{org}/{repo}:clone&#34;,
                relative=f&#34;tf/{doc}&#34;,
                checkout=&#34;clone&#34;,
                backend=backend,
                silent=DEEP,
            )
            WA = WATM(A, nsOrig, skipMeta=skipMeta, extra=extra)
            WA.makeText()
            WA.makeAnno()
            WA.writeAll()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tf.convert.watm.rep"><code class="name flex">
<span>def <span class="ident">rep</span></span>(<span>status)</span>
</code></dt>
<dd>
<div class="desc"><p>Represent a boolean status for a message to the console.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>status</code></strong> :&ensp;<code>boolean</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>string</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L293-L304" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def rep(status):
    &#34;&#34;&#34;Represent a boolean status for a message to the console.

    Parameters
    ----------
    status: boolean

    Returns
    -------
    string
    &#34;&#34;&#34;
    return &#34;OK&#34; if status else &#34;XX&#34;</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tf.convert.watm.WATM"><code class="flex name class">
<span>class <span class="ident">WATM</span></span>
<span>(</span><span>app, nsOrig, skipMeta=False, extra={})</span>
</code></dt>
<dd>
<div class="desc"><p>The export machinery is exposed as a class, wrapped around a TF dataset.</p>
<p>Wrap the WATM exporter around a TF dataset.</p>
<p>Given an already loaded TF dataset, we make an inventory of all data
we need to perform an export to WATM.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>app</code></strong> :&ensp;<code>object</code></dt>
<dd>A loaded TF dataset, as obtained by a call <code>use(&hellip;)</code>.
See <code><a title="tf.app.use" href="../app.html#tf.app.use">use()</a></code></dd>
<dt><strong><code>nsOrig</code></strong> :&ensp;<code>string</code></dt>
<dd>A namespace corresponding to the format of the original, pre-Text-Fabric
representation. For example <code>tei</code> for a TEI corpus, <code>pagexml</code> for a
PageXML corpus. The namespace is not related to XML namespaces, it is
merely a device to categorize the resulting annotations.</dd>
<dt><strong><code>skipMeta</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Only relevant for TEI corpora. If True, all material in the TEI Header
will not be converted to tokens in the text.
More precisely: all TF slots for which the feature <code>is_meta</code> has a true-ish
value will be skipped. If there is no feature <code>is_meta</code> in the dataset,
the setting of <code>skipMeta</code> will have no effect: nothing will be excluded.</dd>
<dt><strong><code>extra</code></strong> :&ensp;<code>dictionary</code>, optional <code>{}</code></dt>
<dd>The data for extra annotations, which will be generated on the fly under the
namespace <code>anno</code>. The keys are the names of features/attributes, the
value for each key is a dictionary that maps nodes to values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L307-L1511" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class WATM:
    &#34;&#34;&#34;The export machinery is exposed as a class, wrapped around a TF dataset.&#34;&#34;&#34;

    def __init__(self, app, nsOrig, skipMeta=False, extra={}):
        &#34;&#34;&#34;Wrap the WATM exporter around a TF dataset.

        Given an already loaded TF dataset, we make an inventory of all data
        we need to perform an export to WATM.

        Parameters
        ----------
        app: object
            A loaded TF dataset, as obtained by a call `use(...)`.
            See `tf.app.use`
        nsOrig: string
            A namespace corresponding to the format of the original, pre-Text-Fabric
            representation. For example `tei` for a TEI corpus, `pagexml` for a
            PageXML corpus. The namespace is not related to XML namespaces, it is
            merely a device to categorize the resulting annotations.
        skipMeta: boolean, optional False
            Only relevant for TEI corpora. If True, all material in the TEI Header
            will not be converted to tokens in the text.
            More precisely: all TF slots for which the feature `is_meta` has a true-ish
            value will be skipped. If there is no feature `is_meta` in the dataset,
            the setting of `skipMeta` will have no effect: nothing will be excluded.
        extra: dictionary, optional {}
            The data for extra annotations, which will be generated on the fly under the
            namespace `anno`. The keys are the names of features/attributes, the
            value for each key is a dictionary that maps nodes to values.
        &#34;&#34;&#34;
        self.app = app
        self.nsOrig = nsOrig
        self.extra = extra
        api = app.api
        F = api.F
        E = api.E
        T = api.T
        sectionTypes = T.sectionTypes

        if len(sectionTypes) == 0:
            console(
                &#34;No section types in corpus. &#34;
                &#34;We need at least one section level for tier-0&#34;,
                error=True,
            )
            self.error = True
        else:
            tierType = T.sectionTypes[0]
            console(f&#34;Tier 0 is section level &#39;{tierType}&#39;&#34;)
            self.tierType = tierType
            self.error = False

        self.L = api.L
        self.Es = api.Es
        self.F = F
        self.E = E
        self.Fs = api.Fs
        self.slotType = self.F.otype.slotType
        self.otypes = self.F.otype.all
        self.info = app.info
        self.repoLocation = app.repoLocation

        Fall = api.Fall
        Eall = api.Eall
        self.Fall = Fall
        self.Eall = Eall

        excludedFeatures = {OTYPE, OSLOTS, &#34;after&#34;, &#34;str&#34;}
        self.nodeFeatures = [f for f in Fall() if f not in excludedFeatures]
        self.edgeFeatures = [f for f in Eall() if f not in excludedFeatures]

        FAllSet = set(Fall())

        self.fotypev = F.otype.v
        self.eoslots = E.oslots.s
        self.emptyv = F.empty.v if &#34;empty&#34; in FAllSet else None
        self.strv = F.str.v if &#34;str&#34; in FAllSet else None
        self.rstrv = F.rstr.v if &#34;rstr&#34; in FAllSet else None
        self.afterv = F.after.v if &#34;after&#34; in FAllSet else None
        self.rafterv = F.rafter.v if &#34;rafter&#34; in FAllSet else None
        is_metav = F.is_meta.v if &#34;is_meta&#34; in FAllSet else None
        self.is_metav = is_metav

        app.dm(f&#34;[WATM exporter docs]({URL_TF_DOCS}/convert/watm.html)&#34;)

        if skipMeta and not is_metav:
            console(
                &#34;skipMeta=True has no effect because feature is_meta is not defined.&#34;,
                error=True,
            )
            skipMeta = False

        self.skipMeta = skipMeta

    def makeText(self):
        &#34;&#34;&#34;Creates the text data.

        The text is a list of tokens and will be stored in member `text` in this object.
        Additionally, the mapping from slot numbers in the TF data
        to indices in this list is stored in member `tlFromTf`.
        &#34;&#34;&#34;
        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        F = self.F
        L = self.L
        slotType = self.slotType
        tierType = self.tierType
        skipMeta = self.skipMeta

        emptyv = self.emptyv
        strv = self.strv
        rstrv = self.rstrv
        afterv = self.afterv
        rafterv = self.rafterv
        is_metav = self.is_metav

        texts = []
        tlFromTf = {}

        self.texts = texts
        self.tlFromTf = tlFromTf

        for ti, sec0 in enumerate(F.otype.s(tierType)):
            text = []
            texts.append(text)

            for s in L.d(sec0, otype=slotType):
                if skipMeta and is_metav(s):
                    continue

                after = rafterv(s) if rafterv else None

                if after is None:
                    after = afterv(s) if afterv else None

                if after is None:
                    after = &#34;&#34;

                if emptyv and emptyv(s):
                    value = after
                else:
                    string = rstrv(s) if rstrv else None

                    if string is None:
                        string = strv(s) if strv else None

                    if string is None:
                        string = &#34;&#34;

                    value = f&#34;{string}{after}&#34;

                text.append(value)
                t = len(text) - 1
                tlFromTf[s] = (ti, t)

    def mkAnno(self, kind, ns, body, target):
        &#34;&#34;&#34;Make a single annotation and return its id.

        Parameters
        ----------
        kind: string
            The kind of annotation.
        ns: string
            The namespace of the annotation.
        body: string
            The body of the annotation.
        target: string  or tuple of strings
            The target of the annotation.
        &#34;&#34;&#34;
        annos = self.annos
        aId = f&#34;a{len(annos):&gt;08}&#34;
        annos.append((kind, aId, ns, body, target))
        return aId

    def makeAnno(self):
        &#34;&#34;&#34;Make all annotations.

        The annotations are stored in a big list, in member `anno` of this object.

        The mapping from slots to indices in the list of tokens is now extended
        with the mapping from nodes to corresponding node annotations.

        So member `tlFromTf` is now a full mapping from all nodes in TF to
        tokens and/or annotations in WATM.
        &#34;&#34;&#34;
        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        Es = self.Es
        F = self.F
        Fs = self.Fs
        fotypev = self.fotypev
        eoslots = self.eoslots
        nodeFeatures = self.nodeFeatures
        edgeFeatures = self.edgeFeatures
        slotType = self.slotType
        otypes = self.otypes
        nsOrig = self.nsOrig
        skipMeta = self.skipMeta
        extra = self.extra

        tlFromTf = self.tlFromTf

        is_metav = self.is_metav

        isTei = nsOrig == NS_TEI

        annos = []
        texts = self.texts
        self.annos = annos

        invertedTargets = []
        farTargets = []

        def mkTarget(n):
            ts = tlFromTf[n]
            return f&#34;{ts[0]}:{ts[1]}-{ts[1] + 1}&#34; if fotypev(n) == slotType else ts

        for otype in otypes:
            isSlot = otype == slotType

            for n in F.otype.s(otype):
                if isSlot:
                    if skipMeta and is_metav(n):
                        continue

                    self.mkAnno(KIND_NODE, NS_TF, n, mkTarget(n))
                else:
                    ws = eoslots(n)
                    if skipMeta and (is_metav(ws[0]) or is_metav(ws[-1])):
                        continue

                    ti0, start = tlFromTf[ws[0]]
                    ti1, end = tlFromTf[ws[-1]]

                    if ti0 != ti1:
                        farTargets.append((otype, ti0, start, ti1, end))
                        continue

                    if end &lt; start:
                        invertedTargets.append((otype, ti0, start, end))
                        start, end = (end, start)

                    target = f&#34;{ti0}:{start}-{end + 1}&#34;
                    aId = (
                        self.mkAnno(KIND_PI, nsOrig, otype[1:], target)
                        if otype.startswith(&#34;?&#34;)
                        else self.mkAnno(
                            KIND_ELEM, NS_FROM_OTYPE.get(otype, nsOrig), otype, target
                        )
                    )
                    tlFromTf[n] = aId
                    self.mkAnno(KIND_NODE, NS_TF, n, aId)

        for feat in nodeFeatures:
            ns = Fs(feat).meta.get(&#34;conversionCode&#34;, NS_FROM_FEAT.get(feat, nsOrig))

            if ns is None:
                console(
                    f&#34;Node feature {feat} has no namespace, &#34;
                    f&#34;defaulting to {NS_NONE}&#34;,
                    error=True,
                )
                ns = NS_NONE

            isRend = False
            isNote = False

            if isTei:
                parts = feat.split(&#34;_&#34;, 2)
                isRend = len(parts) &gt;= 2 and parts[0] == &#34;rend&#34;
                isNote = len(parts) == 2 and parts[0] == &#34;is&#34; and parts[1] == &#34;note&#34;

            if isRend or isNote:
                body = parts[1] if isRend else &#34;note&#34;

                for n, val in Fs(feat).items():
                    if not val or (skipMeta and is_metav(n)):
                        continue

                    self.mkAnno(KIND_FMT, ns, body, mkTarget(n))
            else:
                for n, val in Fs(feat).items():
                    if val is None or skipMeta and is_metav(n):
                        continue

                    body = f&#34;{feat}={val}&#34;
                    self.mkAnno(KIND_ATTR, ns, body, mkTarget(n))

        for feat in edgeFeatures:
            ns = Es(feat).meta.get(&#34;conversionCode&#34;, NS_FROM_FEAT.get(feat, nsOrig))

            if ns is None:
                console(
                    f&#34;Edge feature {feat} has no conversion code, &#34;
                    f&#34;defaulting to {NS_NONE}&#34;,
                    error=True,
                )
                ns = NS_NONE

            for fromNode, toNodes in Es(feat).items():
                if skipMeta and is_metav(fromNode):
                    continue

                if fromNode not in tlFromTf:
                    continue

                targetFrom = mkTarget(fromNode)

                if type(toNodes) is dict:
                    for toNode, val in toNodes.items():
                        if skipMeta and is_metav(toNode):
                            continue

                        if toNode not in tlFromTf:
                            continue

                        body = f&#34;{feat}={val}&#34;
                        targetTo = mkTarget(toNode)
                        target = f&#34;{targetFrom}-&gt;{targetTo}&#34;
                        self.mkAnno(KIND_EDGE, ns, body, target)
                else:
                    for toNode in toNodes:
                        if skipMeta and is_metav(toNode):
                            continue

                        if toNode not in tlFromTf:
                            continue

                        targetTo = mkTarget(toNode)
                        target = f&#34;{targetFrom}-&gt;{targetTo}&#34;
                        self.mkAnno(KIND_EDGE, ns, feat, target)

        for feat, featData in extra.items():
            for n, value in featData.items():
                self.mkAnno(KIND_ANNO, NS_TT, f&#34;{feat}={value}&#34;, mkTarget(n))

        if len(invertedTargets):
            console(f&#34;WARNING: inverted targets, {len(invertedTargets)}x&#34;)
            for otype, ti0, start, end in invertedTargets:
                text = texts[ti0]
                sega = text[start]
                segb = text[end - 1]
                console(f&#34;{otype:&gt;20} {start:&gt;6} `{sega}` &gt; {end - 1} `{segb}`&#34;)

        if len(farTargets):
            console(
                f&#34;ERROR: targets across tier0 items, {len(farTargets)}x&#34;,
                error=True,
            )
            for otype, ti0, start, ti1, end in farTargets:
                sega = texts[ti0][start]
                segb = texts[ti1][end - 1]
                console(
                    f&#34;{otype:&gt;20} {ti0:&gt;2}:{start:&gt;6} `{sega}` - &#34;
                    f&#34;{ti1:&gt;2}:{end - 1} `{segb}`&#34;
                )

    def writeAll(self):
        &#34;&#34;&#34;Write text and annotation data to disk.

        The data will be written as JSON files.
        When the annotation data grows larger than a certain threshold, it will be
        divided over several files.

        The annotations are sorted by annotation id.
        &#34;&#34;&#34;

        # text files

        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        app = self.app
        texts = self.texts
        annos = self.annos

        baseDir = self.repoLocation
        relative = app.context.relative
        version = app.version
        wRelative = REL_RE.sub(f&#34;/{TT_NAME}/{version}/&#34;, relative, count=1)
        resultDir = f&#34;{baseDir}{wRelative}&#34;

        textFiles = []
        self.textFiles = textFiles

        initTree(resultDir, fresh=True)

        total = 0

        for i, text in enumerate(texts):
            textFile = f&#34;{resultDir}/text-{i}.json&#34;
            textFiles.append(textFile)
            nText = len(text)
            total += nText

            with open(textFile, &#34;w&#34;) as fh:
                json.dump(
                    dict(_ordered_segments=text), fh, ensure_ascii=False, indent=1
                )

            console(f&#34;Text file {i:&gt;4}: {nText:&gt;8} segments to {textFile}&#34;)

        nTextFiles = len(textFiles)
        sep = &#34;&#34; if nTextFiles == 1 else &#34;s&#34;
        console(f&#34;Text files all: {total:&gt;8} segments to {nTextFiles} file{sep}&#34;)

        # annotation files

        annoStore = {}

        for kind, aId, ns, body, target in annos:
            annoStore[aId] = (kind, ns, body, target)

        aIdSorted = sorted(annoStore.keys())

        annoFile = f&#34;{resultDir}/anno.tsv&#34;

        if False:
            with open(annoFile, &#34;w&#34;) as fh:
                for aId in aIdSorted:
                    kind, ns, body, target = annoStore[aId]
                    fh.write(f&#34;{aId}\t{kind}\t{ns}\t{body}\t{target}\n&#34;)

        thisAnnoStore = {}
        thisA = 1
        annoFiles = []
        self.annoFiles = annoFiles

        LIMIT = 400000
        j = 0
        total = 0

        def writeThis():
            annoFile = f&#34;{resultDir}/anno-{thisA:&gt;01}.json&#34;
            annoFiles.append(annoFile)

            with open(annoFile, &#34;w&#34;) as fh:
                json.dump(thisAnnoStore, fh, ensure_ascii=False, indent=1)

            console(f&#34;Anno file {i:&gt;4}: {j:&gt;8} annotations written to {annoFile}&#34;)

        for aId in aIdSorted:
            if j &gt;= LIMIT:
                writeThis()
                thisA += 1
                thisAnnoStore = {}
                total += j
                j = 0

            thisAnnoStore[aId] = annoStore[aId]
            j += 1

        if len(thisAnnoStore):
            writeThis()
            total += j

        if len(annos) != total:
            console(f&#34;Sum of batches : {total:&gt;8}&#34;)
            console(f&#34;All annotations: {len(annoStore):&gt;8}&#34;)
            console(&#34;Mismatch in number of annotations&#34;, error=True)

        nAnnoFiles = len(annoFiles)
        sep = &#34;&#34; if nAnnoFiles == 1 else &#34;s&#34;
        console(f&#34;Anno files all: {total:&gt;8} annotations to {nAnnoFiles} file{sep}&#34;)

    @staticmethod
    def compare(nTF, nWA):
        &#34;&#34;&#34;Compare two numbers and report the outcome.

        Used for testing the WATM conversion.

        Parameters
        ----------
        nTF: integer
            The number as it is counted from the original TF dataset.
        nWA: integer
            The number as it is counted from the generated WATM dataset.

        Returns
        -------
        boolean
            Whether the two values are equal.
        &#34;&#34;&#34;
        console(f&#34;\tTF: {nTF:&gt;6}\n\tWA: {nWA:&gt;6}&#34;, error=nTF != nWA)
        return nTF == nWA

    @staticmethod
    def strEqual(wa=None, tf=None):
        &#34;&#34;&#34;Compare two strings and report the outcome.

        Used for testing the WATM conversion.

        Parameters
        ----------
        nTF: string
            The string as encountered in the original TF dataset.
        nWA: string
            The string as encountered in the generated WATM dataset.

        Returns
        -------
        boolean
            Whether the two values are equal.
        &#34;&#34;&#34;
        different = False

        for i, cTF in enumerate(tf):
            if i &gt;= len(wa):
                contextI = max((0, i - 10))
                console(f&#34;\tWA {i}: {wa[contextI:i]} &lt;END&gt;&#34;, error=True)
                console(f&#34;\tTF {i}: {tf[contextI:i]} &lt;&gt; {tf[i:i + 10]}&#34;, error=True)
                different = True
                break
            elif tf[i] != wa[i]:
                contextI = max((0, i - 10))
                console(
                    f&#34;\tWA {i}: {wa[contextI:i]} &lt;{wa[i]}&gt; {wa[i + 1:i + 11]}&#34;,
                    error=True,
                )
                console(
                    f&#34;\tTF {i}: {tf[contextI:i]} &lt;{tf[i]}&gt; {tf[i + 1:i + 11]}&#34;,
                    error=True,
                )
                different = True
                break

        if not different and len(wa) &gt; len(tf):
            i = len(tf)
            contextI = max((0, i - 10))
            console(f&#34;\tWA {i}: {wa[contextI:i]} &lt;&gt; {wa[i:i + 10]}&#34;, error=True)
            console(f&#34;\tTF {i}: {tf[contextI:i]} &lt;END&gt;&#34;, error=True)
            different = True

        sampleWA = f&#34;{wa[0:20]} ... {wa[-20:]}&#34;.replace(&#34;\n&#34;, &#34; &#34;)
        sampleTF = f&#34;{tf[0:20]} ... {tf[-20:]}&#34;.replace(&#34;\n&#34;, &#34; &#34;)
        console(f&#34;\tTF: {sampleTF:&gt;6}\n\tWA: {sampleWA:&gt;6}&#34;)
        return not different

    def testAll(self):
        &#34;&#34;&#34;Test all aspects of the WATM conversion.

        For all kinds of information, such as nodes, edges, features, tokens,
        annotations, we check whether the parts that should correspond between
        the TF dataset and the WATM annotations do so indeed.

        We present some statistics, and highlight the mismatches.

        Returns
        -------
        boolean
            Whether all things that must agree do indeed agree.
        &#34;&#34;&#34;
        error = self.error

        if error:
            console(&#34;Cannot run because of an earlier error&#34;, error=True)

        self.testSetup()

        good = True

        if not self.testText():
            good = False

        if not self.testElements():
            good = False

        if not self.testAttributes():
            good = False

        if not self.testExtra():
            good = False

        if not self.testEdges():
            good = False

        console(&#34;Overall outcome ...&#34;)
        console(f&#34;{rep(good)} - whether all tests passed&#34;, error=not good)

        return good

    def testSetup(self):
        &#34;&#34;&#34;Prepare the tests.

        We read the WATM dataset and store the tokens in member `testTokens`
        and the annotations in the member `testAnnotations`.
        We unpack targets if they contain structured information.
        &#34;&#34;&#34;
        textFiles = self.textFiles
        annoFiles = self.annoFiles

        tokenFiles = []

        for textFile in textFiles:
            with open(textFile) as fh:
                text = json.load(fh)
                tokens = text[&#34;_ordered_segments&#34;]
                tokenFiles.append(tokens)

        self.testTokens = tokenFiles

        annotations = []

        for annoFile in annoFiles:
            with open(annoFile) as fh:
                annos = json.load(fh)

                for aId, (kind, ns, body, target) in annos.items():
                    if &#34;-&gt;&#34; in target:
                        parts = target.split(&#34;-&gt;&#34;, 1)
                    else:
                        parts = [target]

                    newParts = []

                    for part in parts:
                        if &#34;-&#34; in part:
                            file, part = part.split(&#34;:&#34;, 1)
                            start, end = part.split(&#34;-&#34;, 1)
                            part = (int(file), int(start), int(end))

                        newParts.append(part)

                    target = newParts[0] if len(newParts) == 1 else tuple(newParts)

                    annotations.append((aId, kind, body, target))

        annotations = sorted(annotations)
        self.testAnnotations = annotations

    def testText(self):
        &#34;&#34;&#34;Test the text.

        We test the number of tokens and the equality of the resulting text:
        whether the TF and WATM datasets agree on it.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        F = self.F
        skipMeta = self.skipMeta
        is_metav = self.is_metav
        tokenFiles = self.testTokens
        texts = self.texts

        console(&#34;Testing the text ...&#34;)

        nTokensTF = sum(
            0 if skipMeta and is_metav(s) else 1 for s in range(1, F.otype.maxSlot + 1)
        )
        nTokensWA = sum(len(tokens) for tokens in tokenFiles)
        nGood = self.compare(nTokensTF, nTokensWA)
        console(f&#34;{rep(nGood)} - whether the amounts of tokens agree&#34;, error=not nGood)

        textWA = &#34;&#34;.join(&#34;&#34;.join(tokens) for tokens in tokenFiles)
        textTF = &#34;&#34;.join(&#34;&#34;.join(text) for text in texts)

        tGood = self.strEqual(wa=textWA, tf=textTF)
        console(f&#34;{rep(tGood)} - whether the text is the same&#34;, error=not tGood)

        return nGood and tGood

    def testElements(self):
        &#34;&#34;&#34;Test the elements.

        We test the annotations representing elements/processing instructions
        and check whether they correspond 1-1 to the non-slot nodes in the TF
        dataset.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        F = self.F
        fotypev = self.fotypev
        eoslots = self.eoslots
        skipMeta = self.skipMeta
        is_metav = self.is_metav
        annotations = self.testAnnotations

        console(&#34;Testing the elements ...&#34;)

        nElementsTF = 0
        nPisTF = 0

        for n in range(F.otype.maxSlot + 1, F.otype.maxNode + 1):
            nType = fotypev(n)
            isPi = nType.startswith(&#34;?&#34;)

            if isPi:
                nPisTF += 1

            slots = eoslots(n)
            b = slots[0]
            e = slots[-1]

            if skipMeta and (is_metav(b) or is_metav(e)):
                continue
            else:
                if not isPi:
                    nElementsTF += 1

        nElementsWA = sum(1 if a[1] == &#34;element&#34; else 0 for a in annotations)
        nPisWA = sum(1 if a[1] == &#34;pi&#34; else 0 for a in annotations)

        eGood = self.compare(nElementsTF, nElementsWA)
        console(
            f&#34;{rep(eGood)} - whether the amounts of elements and nodes agree&#34;,
            error=not eGood,
        )

        console(&#34;Testing the processing instructions ...&#34;)

        pGood = self.compare(nPisTF, nPisWA)
        console(
            f&#34;{rep(pGood)} - whether the amounts of processing instructions agree&#34;,
            error=not pGood,
        )

        console(&#34;Testing the element annotations ...&#34;)

        tfFromAid = {}

        element = 0
        pi = 0
        other = 0
        good = 0
        wrong = 0
        unmapped = 0

        for aId, kind, body, target in annotations:
            if kind == &#34;node&#34;:
                tfFromAid[target] = body

        self.tfFromAid = tfFromAid

        console(f&#34;\t{len(tfFromAid)} element annotations&#34;)

        for aId, kind, body, target in annotations:
            isElem = kind == &#34;element&#34;
            isPi = kind == &#34;pi&#34;

            if not isElem and not isPi:
                other += 1
                continue

            if isElem:
                element += 1
            else:
                pi += 1

            tag = body
            node = tfFromAid.get(aId, None)
            if node is None:
                unmapped += 1
                continue

            otype = fotypev(node)

            if isPi and tag == otype[1:] or not isPi and tag == otype:
                good += 1
            else:
                wrong += 1

        console(f&#34;\tElement : {element:&gt;5} x&#34;)
        console(f&#34;\tPi      : {pi:&gt;5} x&#34;)
        console(f&#34;\tOther   : {other:&gt;5} x&#34;)
        console(f&#34;\tGood    : {good:&gt;5} x&#34;)
        console(f&#34;\tWrong   : {wrong:&gt;5} x&#34;)
        console(f&#34;\tUnmapped: {unmapped:&gt;5} x&#34;)

        aGood = wrong == 0 and unmapped == 0
        console(
            f&#34;{rep(aGood)} - whether all element annotations are ok&#34;, error=not aGood
        )

        return aGood and eGood and pGood

    def testAttributes(self):
        &#34;&#34;&#34;Test the attributes.

        We test whether attributes and features correspond to each other.

        Some attributes in the original TEI are converted in a special way into
        TF features: this holds for the `rend` attribute.
        Basically, a value `rend=&#34;italic&#34;` is translated into feature
        `is_italic=1`.
        In turn, these features have been translated into annotations of kind
        `format`. We test them separately.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        Fs = self.Fs
        Fall = self.Fall
        eoslots = self.eoslots
        skipMeta = self.skipMeta
        is_metav = self.is_metav
        annotations = self.testAnnotations
        tfFromAid = self.tfFromAid
        nsOrig = self.nsOrig

        isTei = nsOrig == NS_TEI

        console(&#34;Testing the attributes ...&#34;)

        attWA = []

        for aId, kind, body, target in annotations:
            if kind != &#34;attribute&#34;:
                continue
            node = tfFromAid[target]
            att, value = body.split(&#34;=&#34;, 1)
            attWA.append((node, att, value))

        attWA = sorted(attWA)

        console(f&#34;\t{len(attWA)} attribute values&#34;)

        good = 0
        wrong = []

        for node, att, valWA in attWA:
            valTF = str(Fs(att).v(node))
            if valWA == valTF:
                good += 1
            else:
                wrong.append((node, att, valWA, valTF))

        console(f&#34;\tGood:     {good:&gt;5} x&#34;)
        console(f&#34;\tWrong:    {len(wrong):&gt;5} x&#34;)
        consistent = len(wrong) == 0

        console(
            f&#34;{rep(consistent)} - whether annotations are consistent with features&#34;,
            error=not consistent,
        )

        attTF = []

        for feat in Fall():
            if feat in {&#34;otype&#34;, &#34;str&#34;, &#34;after&#34;}:
                continue

            if skipMeta and feat == &#34;is_meta&#34;:
                continue

            if isTei and (
                (feat != &#34;is_meta&#34; and feat.startswith(&#34;is_&#34;))
                or feat.startswith(&#34;rend_&#34;)
            ):
                continue

            for node, valTF in Fs(feat).items():
                slots = eoslots(node)
                b = slots[0]
                e = slots[-1]

                if skipMeta and (is_metav(b) or is_metav(e)):
                    continue

                attTF.append((node, feat, str(valTF)))

        attTF = sorted(attTF)

        console(f&#34;\tWA attributes: {len(attWA)}&#34;)
        console(f&#34;\tTF attributes: {len(attTF)}&#34;)
        complete = attTF == attWA
        console(
            f&#34;{rep(complete)} - whether annotations are complete w.r.t. features&#34;,
            error=not complete,
        )

        console(&#34;Testing the format attributes ...&#34;)

        fmtWA = []

        for aId, kind, body, target in annotations:
            if kind != &#34;format&#34;:
                continue
            if body == &#34;note&#34;:
                continue
            node = tfFromAid[target]
            fmtWA.append((node, body))

        fmtWA = sorted(fmtWA)
        fmtWaFreq = collections.Counter()

        for node, body in fmtWA:
            fmtWaFreq[body] += 1

        console(f&#34;\t{len(fmtWA)} format values&#34;)
        console(&#34;\tformatting attributes: &#34;)
        for fa, n in sorted(fmtWaFreq.items(), key=lambda x: (-x[1], x[0])):
            console(f&#34;\t\t{n:&gt;6} x {fa}&#34;)

        good = 0
        wrong = []

        for node, valWA in fmtWA:
            feat = f&#34;rend_{valWA}&#34;
            valTF = valWA if str(Fs(feat).v(node)) else None
            if valWA == valTF:
                good += 1
            else:
                wrong.append((node, feat, valWA, valTF))

        console(f&#34;\tGood:     {good:&gt;5} x&#34;)
        console(f&#34;\tWrong:    {len(wrong):&gt;5} x&#34;)
        fconsistent = len(wrong) == 0
        console(
            f&#34;{rep(fconsistent)} - &#34;
            f&#34;whether format annotationsare consistent with features&#34;,
            error=not fconsistent,
        )

        fmtTF = []

        for feat in Fall():
            if not feat.startswith(&#34;rend_&#34;):
                continue

            value = feat.split(&#34;_&#34;, 2)[1]
            if value == &#34;note&#34;:
                continue

            for node, valTF in Fs(feat).items():
                slots = eoslots(node)
                b = slots[0]
                e = slots[-1]

                if skipMeta and (is_metav(b) or is_metav(e)):
                    continue

                fmtTF.append((node, value))

        fmtTF = sorted(fmtTF)

        console(f&#34;\tWA format attributes: {len(fmtWA)}&#34;)
        console(f&#34;\tTF format attributes: {len(fmtTF)}&#34;)
        fcomplete = fmtTF == fmtWA
        console(
            f&#34;{rep(complete)} - &#34;
            f&#34;whether format annotations are complete w.r.t. features&#34;,
            error=not fcomplete,
        )

        return consistent and complete and fconsistent and fcomplete

    def testExtra(self):
        &#34;&#34;&#34;Test the extra data for on-the-fly annotations.

        Annotations that have been generated out of the data stored in the
        `extra` parameter with which the object has been initialized, all got
        the kind `anno`.

        Now we check these annotations against the data that went into it.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        annotations = self.testAnnotations
        tfFromAid = self.tfFromAid
        extra = self.extra

        console(&#34;Testing the extra annotations ...&#34;)

        attWA = []

        for aId, kind, body, target in annotations:
            if kind != &#34;anno&#34;:
                continue
            node = tfFromAid[target]
            att, value = body.split(&#34;=&#34;, 1)
            attWA.append((node, att, value))

        attWA = sorted(attWA)

        attEX = []

        for feat, featData in extra.items():
            for n, value in featData.items():
                attEX.append((n, feat, value))

        attEX = sorted(attEX)

        console(f&#34;\t{len(attEX)} extra feature values&#34;)
        console(f&#34;\t{len(attWA)} extra annotations&#34;)

        good = attWA == attEX

        def showData(tuples, isin, isout):
            data = {}

            for n, f, v in tuples:
                data.setdefault(f, {})[n] = v

            for f in sorted(data):
                fData = data[f]
                console(
                    f&#34;\t{isin}: {f} misses {len(fData)} annotations in {isout}&#34;,
                    error=True,
                )
                for n in sorted(fData.keys())[0:3]:
                    console(f&#34;\t\t\t{n:&gt;7} = {fData[n]}&#34;, error=True)

        if not good:
            attWASet = set(attWA)
            attEXSet = set(attEX)

            onlyWA = attWASet - attEXSet
            onlyEX = attEXSet - attWASet

            if len(onlyWA):
                showData(onlyWA, &#34;WA&#34;, &#34;EX&#34;)
            else:
                console(&#34;\tWA: All extra annotations derive from the extra data&#34;)
            if len(onlyEX):
                showData(onlyEX, &#34;EX&#34;, &#34;WA&#34;)
            else:
                console(&#34;\tEX: All extra data ended up as annotations&#34;)

        console(f&#34;{rep(good)} - whether the extra annotations agree&#34;, error=not good)

        return good

    def testEdges(self):
        &#34;&#34;&#34;Test the edges.

        Edges in TF are links between nodes, and they translate into annotations of
        kind `edge` which target a pair of annotations: the `from` annotation,
        and the `to` annotation.

        Here we check whether the TF edges are faithfully and completely parallelled
        by annotations.

        Returns
        -------
        boolean
            Whether all these tests succeed.
        &#34;&#34;&#34;
        Es = self.Es
        Eall = self.Eall
        annotations = self.testAnnotations

        console(&#34;Testing the edges ...&#34;)

        tfFromWANodes = {}
        tfFromWAEdges = {}

        for aId, kind, body, target in annotations:
            if kind != &#34;node&#34;:
                continue
            if type(target) is tuple:
                file, start, end = target
                if start + 1 != end:
                    # we expect that node annotations either targets a single token
                    # or an element/pi annotation
                    print(target)
                    break
                target = (file, end)
            tfFromWANodes[target] = body

        for aId, kind, body, target in annotations:
            if kind != &#34;edge&#34;:
                continue

            fro, to = target
            fromNode = tfFromWANodes[fro]
            toNode = tfFromWANodes[to]
            parts = body.split(&#34;=&#34;, 1)
            name, val = (body, None) if len(parts) == 1 else parts
            tfFromWAEdges.setdefault(name, {}).setdefault(fromNode, {})[toNode] = val

        console(f&#34;\tFound: {len(tfFromWANodes)} nodes&#34;)

        for edge, edgeData in sorted(tfFromWAEdges.items()):
            console(f&#34;\tFound edge {edge} with {len(edgeData)} starting nodes&#34;)

        allGood = True

        for edge in set(Eall()) | set(tfFromWAEdges):
            if edge == &#34;oslots&#34;:
                continue

            console(f&#34;\tChecking edge {edge}&#34;)

            good = True

            if edge not in set(Eall()):
                console(&#34;\t\tmissing in TF data&#34;, error=True)
                good = False

            if edge not in tfFromWAEdges:
                console(&#34;\t\tmissing in annotation data&#34;, error=True)
                good = False

            if not good:
                continue

            dataTF = dict(Es(edge).items())
            dataWA = tfFromWAEdges[edge]

            fromNodesTF = set(dataTF)
            fromNodesWA = set(dataWA)

            nFromTF = len(fromNodesTF)
            nFromWA = len(fromNodesWA)

            if fromNodesTF == fromNodesWA:
                console(f&#34;\t\tsame {nFromTF} fromNodes&#34;)
            else:
                console(
                    f&#34;\t\tfrom nodes differ: {nFromTF} in TF, {nFromWA} in WA&#34;,
                    error=True,
                )
                good = False

            diffs = []

            nToChecked = 0

            for f, toNodeInfoTF in dataTF.items():
                toNodeInfoWA = dataWA[f]
                if type(toNodeInfoTF) is dict:
                    toNodeInfoTF = {k: str(v) for (k, v) in toNodeInfoTF.items()}
                else:
                    toNodeInfoTF = {x: None for x in toNodeInfoTF}

                if toNodeInfoTF != toNodeInfoWA:
                    diffs.append((f, toNodeInfoTF, toNodeInfoWA))

                nToChecked += len(toNodeInfoTF)

            if len(diffs):
                good = False
                console(
                    f&#34;\t\tdifferences in toNodes for {len(diffs)} fromNodes&#34;, error=True
                )

                for f, toNodeInfoTF, toNodeInfoWA in sorted(diffs)[0:10]:
                    console(f&#34;\t\t\tfromNode {f}&#34;, error=True)

                    toNodesTF = set(toNodeInfoTF)
                    toNodesWA = set(toNodeInfoWA)

                    nToTF = len(toNodesTF)
                    nToWA = len(toNodesWA)

                    if toNodesTF == toNodesWA:
                        console(f&#34;\t\t\tsame {nToTF} toNodes&#34;)
                    else:
                        console(
                            f&#34;\t\t\ttoNodes differ: {nToTF} in TF, {nToWA} in WA&#34;,
                            error=True,
                        )
                    for t in toNodesTF | toNodesWA:
                        doCompare = True
                        if t not in toNodesTF:
                            console(f&#34;\t\t\t\ttoNode {t} not in TF&#34;, error=True)
                            doCompare = False
                        else:
                            valTF = toNodeInfoTF[t]

                        if t not in toNodesWA:
                            console(f&#34;\t\t\t\ttoNode {t} not in WA&#34;, error=True)
                            doCompare = False
                        else:
                            valWA = toNodeInfoWA[t]

                        if doCompare:
                            if valTF == valWA:
                                console(
                                    f&#34;\t\t\t\ttoNode{t} values agree: {repr(valTF)}&#34;
                                )
                            else:
                                console(
                                    f&#34;\t\t\t\ttoNode{t} values differ: &#34;
                                    f&#34;TF: {repr(valTF)} WA: {repr(valWA)}&#34;,
                                    error=True,
                                )

            console(f&#34;\t{rep(good)} - {nToChecked} toNodes checked&#34;, error=not good)

            if not good:
                allGood = False

        console(f&#34;{rep(allGood)} - whether all edges agree&#34;)

        return allGood</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="tf.convert.watm.WATM.compare"><code class="name flex">
<span>def <span class="ident">compare</span></span>(<span>nTF, nWA)</span>
</code></dt>
<dd>
<div class="desc"><p>Compare two numbers and report the outcome.</p>
<p>Used for testing the WATM conversion.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>nTF</code></strong> :&ensp;<code>integer</code></dt>
<dd>The number as it is counted from the original TF dataset.</dd>
<dt><strong><code>nWA</code></strong> :&ensp;<code>integer</code></dt>
<dd>The number as it is counted from the generated WATM dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether the two values are equal.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L780-L799" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def compare(nTF, nWA):
    &#34;&#34;&#34;Compare two numbers and report the outcome.

    Used for testing the WATM conversion.

    Parameters
    ----------
    nTF: integer
        The number as it is counted from the original TF dataset.
    nWA: integer
        The number as it is counted from the generated WATM dataset.

    Returns
    -------
    boolean
        Whether the two values are equal.
    &#34;&#34;&#34;
    console(f&#34;\tTF: {nTF:&gt;6}\n\tWA: {nWA:&gt;6}&#34;, error=nTF != nWA)
    return nTF == nWA</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.strEqual"><code class="name flex">
<span>def <span class="ident">strEqual</span></span>(<span>wa=None, tf=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compare two strings and report the outcome.</p>
<p>Used for testing the WATM conversion.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>nTF</code></strong> :&ensp;<code>string</code></dt>
<dd>The string as encountered in the original TF dataset.</dd>
<dt><strong><code>nWA</code></strong> :&ensp;<code>string</code></dt>
<dd>The string as encountered in the generated WATM dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether the two values are equal.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L801-L851" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def strEqual(wa=None, tf=None):
    &#34;&#34;&#34;Compare two strings and report the outcome.

    Used for testing the WATM conversion.

    Parameters
    ----------
    nTF: string
        The string as encountered in the original TF dataset.
    nWA: string
        The string as encountered in the generated WATM dataset.

    Returns
    -------
    boolean
        Whether the two values are equal.
    &#34;&#34;&#34;
    different = False

    for i, cTF in enumerate(tf):
        if i &gt;= len(wa):
            contextI = max((0, i - 10))
            console(f&#34;\tWA {i}: {wa[contextI:i]} &lt;END&gt;&#34;, error=True)
            console(f&#34;\tTF {i}: {tf[contextI:i]} &lt;&gt; {tf[i:i + 10]}&#34;, error=True)
            different = True
            break
        elif tf[i] != wa[i]:
            contextI = max((0, i - 10))
            console(
                f&#34;\tWA {i}: {wa[contextI:i]} &lt;{wa[i]}&gt; {wa[i + 1:i + 11]}&#34;,
                error=True,
            )
            console(
                f&#34;\tTF {i}: {tf[contextI:i]} &lt;{tf[i]}&gt; {tf[i + 1:i + 11]}&#34;,
                error=True,
            )
            different = True
            break

    if not different and len(wa) &gt; len(tf):
        i = len(tf)
        contextI = max((0, i - 10))
        console(f&#34;\tWA {i}: {wa[contextI:i]} &lt;&gt; {wa[i:i + 10]}&#34;, error=True)
        console(f&#34;\tTF {i}: {tf[contextI:i]} &lt;END&gt;&#34;, error=True)
        different = True

    sampleWA = f&#34;{wa[0:20]} ... {wa[-20:]}&#34;.replace(&#34;\n&#34;, &#34; &#34;)
    sampleTF = f&#34;{tf[0:20]} ... {tf[-20:]}&#34;.replace(&#34;\n&#34;, &#34; &#34;)
    console(f&#34;\tTF: {sampleTF:&gt;6}\n\tWA: {sampleWA:&gt;6}&#34;)
    return not different</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="tf.convert.watm.WATM.makeAnno"><code class="name flex">
<span>def <span class="ident">makeAnno</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Make all annotations.</p>
<p>The annotations are stored in a big list, in member <code>anno</code> of this object.</p>
<p>The mapping from slots to indices in the list of tokens is now extended
with the mapping from nodes to corresponding node annotations.</p>
<p>So member <code>tlFromTf</code> is now a full mapping from all nodes in TF to
tokens and/or annotations in WATM.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L484-L668" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def makeAnno(self):
    &#34;&#34;&#34;Make all annotations.

    The annotations are stored in a big list, in member `anno` of this object.

    The mapping from slots to indices in the list of tokens is now extended
    with the mapping from nodes to corresponding node annotations.

    So member `tlFromTf` is now a full mapping from all nodes in TF to
    tokens and/or annotations in WATM.
    &#34;&#34;&#34;
    error = self.error

    if error:
        console(&#34;Cannot run because of an earlier error&#34;, error=True)

    Es = self.Es
    F = self.F
    Fs = self.Fs
    fotypev = self.fotypev
    eoslots = self.eoslots
    nodeFeatures = self.nodeFeatures
    edgeFeatures = self.edgeFeatures
    slotType = self.slotType
    otypes = self.otypes
    nsOrig = self.nsOrig
    skipMeta = self.skipMeta
    extra = self.extra

    tlFromTf = self.tlFromTf

    is_metav = self.is_metav

    isTei = nsOrig == NS_TEI

    annos = []
    texts = self.texts
    self.annos = annos

    invertedTargets = []
    farTargets = []

    def mkTarget(n):
        ts = tlFromTf[n]
        return f&#34;{ts[0]}:{ts[1]}-{ts[1] + 1}&#34; if fotypev(n) == slotType else ts

    for otype in otypes:
        isSlot = otype == slotType

        for n in F.otype.s(otype):
            if isSlot:
                if skipMeta and is_metav(n):
                    continue

                self.mkAnno(KIND_NODE, NS_TF, n, mkTarget(n))
            else:
                ws = eoslots(n)
                if skipMeta and (is_metav(ws[0]) or is_metav(ws[-1])):
                    continue

                ti0, start = tlFromTf[ws[0]]
                ti1, end = tlFromTf[ws[-1]]

                if ti0 != ti1:
                    farTargets.append((otype, ti0, start, ti1, end))
                    continue

                if end &lt; start:
                    invertedTargets.append((otype, ti0, start, end))
                    start, end = (end, start)

                target = f&#34;{ti0}:{start}-{end + 1}&#34;
                aId = (
                    self.mkAnno(KIND_PI, nsOrig, otype[1:], target)
                    if otype.startswith(&#34;?&#34;)
                    else self.mkAnno(
                        KIND_ELEM, NS_FROM_OTYPE.get(otype, nsOrig), otype, target
                    )
                )
                tlFromTf[n] = aId
                self.mkAnno(KIND_NODE, NS_TF, n, aId)

    for feat in nodeFeatures:
        ns = Fs(feat).meta.get(&#34;conversionCode&#34;, NS_FROM_FEAT.get(feat, nsOrig))

        if ns is None:
            console(
                f&#34;Node feature {feat} has no namespace, &#34;
                f&#34;defaulting to {NS_NONE}&#34;,
                error=True,
            )
            ns = NS_NONE

        isRend = False
        isNote = False

        if isTei:
            parts = feat.split(&#34;_&#34;, 2)
            isRend = len(parts) &gt;= 2 and parts[0] == &#34;rend&#34;
            isNote = len(parts) == 2 and parts[0] == &#34;is&#34; and parts[1] == &#34;note&#34;

        if isRend or isNote:
            body = parts[1] if isRend else &#34;note&#34;

            for n, val in Fs(feat).items():
                if not val or (skipMeta and is_metav(n)):
                    continue

                self.mkAnno(KIND_FMT, ns, body, mkTarget(n))
        else:
            for n, val in Fs(feat).items():
                if val is None or skipMeta and is_metav(n):
                    continue

                body = f&#34;{feat}={val}&#34;
                self.mkAnno(KIND_ATTR, ns, body, mkTarget(n))

    for feat in edgeFeatures:
        ns = Es(feat).meta.get(&#34;conversionCode&#34;, NS_FROM_FEAT.get(feat, nsOrig))

        if ns is None:
            console(
                f&#34;Edge feature {feat} has no conversion code, &#34;
                f&#34;defaulting to {NS_NONE}&#34;,
                error=True,
            )
            ns = NS_NONE

        for fromNode, toNodes in Es(feat).items():
            if skipMeta and is_metav(fromNode):
                continue

            if fromNode not in tlFromTf:
                continue

            targetFrom = mkTarget(fromNode)

            if type(toNodes) is dict:
                for toNode, val in toNodes.items():
                    if skipMeta and is_metav(toNode):
                        continue

                    if toNode not in tlFromTf:
                        continue

                    body = f&#34;{feat}={val}&#34;
                    targetTo = mkTarget(toNode)
                    target = f&#34;{targetFrom}-&gt;{targetTo}&#34;
                    self.mkAnno(KIND_EDGE, ns, body, target)
            else:
                for toNode in toNodes:
                    if skipMeta and is_metav(toNode):
                        continue

                    if toNode not in tlFromTf:
                        continue

                    targetTo = mkTarget(toNode)
                    target = f&#34;{targetFrom}-&gt;{targetTo}&#34;
                    self.mkAnno(KIND_EDGE, ns, feat, target)

    for feat, featData in extra.items():
        for n, value in featData.items():
            self.mkAnno(KIND_ANNO, NS_TT, f&#34;{feat}={value}&#34;, mkTarget(n))

    if len(invertedTargets):
        console(f&#34;WARNING: inverted targets, {len(invertedTargets)}x&#34;)
        for otype, ti0, start, end in invertedTargets:
            text = texts[ti0]
            sega = text[start]
            segb = text[end - 1]
            console(f&#34;{otype:&gt;20} {start:&gt;6} `{sega}` &gt; {end - 1} `{segb}`&#34;)

    if len(farTargets):
        console(
            f&#34;ERROR: targets across tier0 items, {len(farTargets)}x&#34;,
            error=True,
        )
        for otype, ti0, start, ti1, end in farTargets:
            sega = texts[ti0][start]
            segb = texts[ti1][end - 1]
            console(
                f&#34;{otype:&gt;20} {ti0:&gt;2}:{start:&gt;6} `{sega}` - &#34;
                f&#34;{ti1:&gt;2}:{end - 1} `{segb}`&#34;
            )</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.makeText"><code class="name flex">
<span>def <span class="ident">makeText</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the text data.</p>
<p>The text is a list of tokens and will be stored in member <code>text</code> in this object.
Additionally, the mapping from slot numbers in the TF data
to indices in this list is stored in member <code>tlFromTf</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L401-L463" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def makeText(self):
    &#34;&#34;&#34;Creates the text data.

    The text is a list of tokens and will be stored in member `text` in this object.
    Additionally, the mapping from slot numbers in the TF data
    to indices in this list is stored in member `tlFromTf`.
    &#34;&#34;&#34;
    error = self.error

    if error:
        console(&#34;Cannot run because of an earlier error&#34;, error=True)

    F = self.F
    L = self.L
    slotType = self.slotType
    tierType = self.tierType
    skipMeta = self.skipMeta

    emptyv = self.emptyv
    strv = self.strv
    rstrv = self.rstrv
    afterv = self.afterv
    rafterv = self.rafterv
    is_metav = self.is_metav

    texts = []
    tlFromTf = {}

    self.texts = texts
    self.tlFromTf = tlFromTf

    for ti, sec0 in enumerate(F.otype.s(tierType)):
        text = []
        texts.append(text)

        for s in L.d(sec0, otype=slotType):
            if skipMeta and is_metav(s):
                continue

            after = rafterv(s) if rafterv else None

            if after is None:
                after = afterv(s) if afterv else None

            if after is None:
                after = &#34;&#34;

            if emptyv and emptyv(s):
                value = after
            else:
                string = rstrv(s) if rstrv else None

                if string is None:
                    string = strv(s) if strv else None

                if string is None:
                    string = &#34;&#34;

                value = f&#34;{string}{after}&#34;

            text.append(value)
            t = len(text) - 1
            tlFromTf[s] = (ti, t)</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.mkAnno"><code class="name flex">
<span>def <span class="ident">mkAnno</span></span>(<span>self, kind, ns, body, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a single annotation and return its id.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kind</code></strong> :&ensp;<code>string</code></dt>
<dd>The kind of annotation.</dd>
<dt><strong><code>ns</code></strong> :&ensp;<code>string</code></dt>
<dd>The namespace of the annotation.</dd>
<dt><strong><code>body</code></strong> :&ensp;<code>string</code></dt>
<dd>The body of the annotation.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>string</code>
or <code>tuple</code> of <code>strings</code></dt>
<dd>The target of the annotation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L465-L482" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def mkAnno(self, kind, ns, body, target):
    &#34;&#34;&#34;Make a single annotation and return its id.

    Parameters
    ----------
    kind: string
        The kind of annotation.
    ns: string
        The namespace of the annotation.
    body: string
        The body of the annotation.
    target: string  or tuple of strings
        The target of the annotation.
    &#34;&#34;&#34;
    annos = self.annos
    aId = f&#34;a{len(annos):&gt;08}&#34;
    annos.append((kind, aId, ns, body, target))
    return aId</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testAll"><code class="name flex">
<span>def <span class="ident">testAll</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Test all aspects of the WATM conversion.</p>
<p>For all kinds of information, such as nodes, edges, features, tokens,
annotations, we check whether the parts that should correspond between
the TF dataset and the WATM annotations do so indeed.</p>
<p>We present some statistics, and highlight the mismatches.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all things that must agree do indeed agree.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L853-L894" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testAll(self):
    &#34;&#34;&#34;Test all aspects of the WATM conversion.

    For all kinds of information, such as nodes, edges, features, tokens,
    annotations, we check whether the parts that should correspond between
    the TF dataset and the WATM annotations do so indeed.

    We present some statistics, and highlight the mismatches.

    Returns
    -------
    boolean
        Whether all things that must agree do indeed agree.
    &#34;&#34;&#34;
    error = self.error

    if error:
        console(&#34;Cannot run because of an earlier error&#34;, error=True)

    self.testSetup()

    good = True

    if not self.testText():
        good = False

    if not self.testElements():
        good = False

    if not self.testAttributes():
        good = False

    if not self.testExtra():
        good = False

    if not self.testEdges():
        good = False

    console(&#34;Overall outcome ...&#34;)
    console(f&#34;{rep(good)} - whether all tests passed&#34;, error=not good)

    return good</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testAttributes"><code class="name flex">
<span>def <span class="ident">testAttributes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the attributes.</p>
<p>We test whether attributes and features correspond to each other.</p>
<p>Some attributes in the original TEI are converted in a special way into
TF features: this holds for the <code>rend</code> attribute.
Basically, a value <code>rend="italic"</code> is translated into feature
<code>is_italic=1</code>.
In turn, these features have been translated into annotations of kind
<code>format</code>. We test them separately.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all these tests succeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L1096-L1267" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testAttributes(self):
    &#34;&#34;&#34;Test the attributes.

    We test whether attributes and features correspond to each other.

    Some attributes in the original TEI are converted in a special way into
    TF features: this holds for the `rend` attribute.
    Basically, a value `rend=&#34;italic&#34;` is translated into feature
    `is_italic=1`.
    In turn, these features have been translated into annotations of kind
    `format`. We test them separately.

    Returns
    -------
    boolean
        Whether all these tests succeed.
    &#34;&#34;&#34;
    Fs = self.Fs
    Fall = self.Fall
    eoslots = self.eoslots
    skipMeta = self.skipMeta
    is_metav = self.is_metav
    annotations = self.testAnnotations
    tfFromAid = self.tfFromAid
    nsOrig = self.nsOrig

    isTei = nsOrig == NS_TEI

    console(&#34;Testing the attributes ...&#34;)

    attWA = []

    for aId, kind, body, target in annotations:
        if kind != &#34;attribute&#34;:
            continue
        node = tfFromAid[target]
        att, value = body.split(&#34;=&#34;, 1)
        attWA.append((node, att, value))

    attWA = sorted(attWA)

    console(f&#34;\t{len(attWA)} attribute values&#34;)

    good = 0
    wrong = []

    for node, att, valWA in attWA:
        valTF = str(Fs(att).v(node))
        if valWA == valTF:
            good += 1
        else:
            wrong.append((node, att, valWA, valTF))

    console(f&#34;\tGood:     {good:&gt;5} x&#34;)
    console(f&#34;\tWrong:    {len(wrong):&gt;5} x&#34;)
    consistent = len(wrong) == 0

    console(
        f&#34;{rep(consistent)} - whether annotations are consistent with features&#34;,
        error=not consistent,
    )

    attTF = []

    for feat in Fall():
        if feat in {&#34;otype&#34;, &#34;str&#34;, &#34;after&#34;}:
            continue

        if skipMeta and feat == &#34;is_meta&#34;:
            continue

        if isTei and (
            (feat != &#34;is_meta&#34; and feat.startswith(&#34;is_&#34;))
            or feat.startswith(&#34;rend_&#34;)
        ):
            continue

        for node, valTF in Fs(feat).items():
            slots = eoslots(node)
            b = slots[0]
            e = slots[-1]

            if skipMeta and (is_metav(b) or is_metav(e)):
                continue

            attTF.append((node, feat, str(valTF)))

    attTF = sorted(attTF)

    console(f&#34;\tWA attributes: {len(attWA)}&#34;)
    console(f&#34;\tTF attributes: {len(attTF)}&#34;)
    complete = attTF == attWA
    console(
        f&#34;{rep(complete)} - whether annotations are complete w.r.t. features&#34;,
        error=not complete,
    )

    console(&#34;Testing the format attributes ...&#34;)

    fmtWA = []

    for aId, kind, body, target in annotations:
        if kind != &#34;format&#34;:
            continue
        if body == &#34;note&#34;:
            continue
        node = tfFromAid[target]
        fmtWA.append((node, body))

    fmtWA = sorted(fmtWA)
    fmtWaFreq = collections.Counter()

    for node, body in fmtWA:
        fmtWaFreq[body] += 1

    console(f&#34;\t{len(fmtWA)} format values&#34;)
    console(&#34;\tformatting attributes: &#34;)
    for fa, n in sorted(fmtWaFreq.items(), key=lambda x: (-x[1], x[0])):
        console(f&#34;\t\t{n:&gt;6} x {fa}&#34;)

    good = 0
    wrong = []

    for node, valWA in fmtWA:
        feat = f&#34;rend_{valWA}&#34;
        valTF = valWA if str(Fs(feat).v(node)) else None
        if valWA == valTF:
            good += 1
        else:
            wrong.append((node, feat, valWA, valTF))

    console(f&#34;\tGood:     {good:&gt;5} x&#34;)
    console(f&#34;\tWrong:    {len(wrong):&gt;5} x&#34;)
    fconsistent = len(wrong) == 0
    console(
        f&#34;{rep(fconsistent)} - &#34;
        f&#34;whether format annotationsare consistent with features&#34;,
        error=not fconsistent,
    )

    fmtTF = []

    for feat in Fall():
        if not feat.startswith(&#34;rend_&#34;):
            continue

        value = feat.split(&#34;_&#34;, 2)[1]
        if value == &#34;note&#34;:
            continue

        for node, valTF in Fs(feat).items():
            slots = eoslots(node)
            b = slots[0]
            e = slots[-1]

            if skipMeta and (is_metav(b) or is_metav(e)):
                continue

            fmtTF.append((node, value))

    fmtTF = sorted(fmtTF)

    console(f&#34;\tWA format attributes: {len(fmtWA)}&#34;)
    console(f&#34;\tTF format attributes: {len(fmtTF)}&#34;)
    fcomplete = fmtTF == fmtWA
    console(
        f&#34;{rep(complete)} - &#34;
        f&#34;whether format annotations are complete w.r.t. features&#34;,
        error=not fcomplete,
    )

    return consistent and complete and fconsistent and fcomplete</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testEdges"><code class="name flex">
<span>def <span class="ident">testEdges</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the edges.</p>
<p>Edges in TF are links between nodes, and they translate into annotations of
kind <code>edge</code> which target a pair of annotations: the <code>from</code> annotation,
and the <code>to</code> annotation.</p>
<p>Here we check whether the TF edges are faithfully and completely parallelled
by annotations.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all these tests succeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L1348-L1511" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testEdges(self):
    &#34;&#34;&#34;Test the edges.

    Edges in TF are links between nodes, and they translate into annotations of
    kind `edge` which target a pair of annotations: the `from` annotation,
    and the `to` annotation.

    Here we check whether the TF edges are faithfully and completely parallelled
    by annotations.

    Returns
    -------
    boolean
        Whether all these tests succeed.
    &#34;&#34;&#34;
    Es = self.Es
    Eall = self.Eall
    annotations = self.testAnnotations

    console(&#34;Testing the edges ...&#34;)

    tfFromWANodes = {}
    tfFromWAEdges = {}

    for aId, kind, body, target in annotations:
        if kind != &#34;node&#34;:
            continue
        if type(target) is tuple:
            file, start, end = target
            if start + 1 != end:
                # we expect that node annotations either targets a single token
                # or an element/pi annotation
                print(target)
                break
            target = (file, end)
        tfFromWANodes[target] = body

    for aId, kind, body, target in annotations:
        if kind != &#34;edge&#34;:
            continue

        fro, to = target
        fromNode = tfFromWANodes[fro]
        toNode = tfFromWANodes[to]
        parts = body.split(&#34;=&#34;, 1)
        name, val = (body, None) if len(parts) == 1 else parts
        tfFromWAEdges.setdefault(name, {}).setdefault(fromNode, {})[toNode] = val

    console(f&#34;\tFound: {len(tfFromWANodes)} nodes&#34;)

    for edge, edgeData in sorted(tfFromWAEdges.items()):
        console(f&#34;\tFound edge {edge} with {len(edgeData)} starting nodes&#34;)

    allGood = True

    for edge in set(Eall()) | set(tfFromWAEdges):
        if edge == &#34;oslots&#34;:
            continue

        console(f&#34;\tChecking edge {edge}&#34;)

        good = True

        if edge not in set(Eall()):
            console(&#34;\t\tmissing in TF data&#34;, error=True)
            good = False

        if edge not in tfFromWAEdges:
            console(&#34;\t\tmissing in annotation data&#34;, error=True)
            good = False

        if not good:
            continue

        dataTF = dict(Es(edge).items())
        dataWA = tfFromWAEdges[edge]

        fromNodesTF = set(dataTF)
        fromNodesWA = set(dataWA)

        nFromTF = len(fromNodesTF)
        nFromWA = len(fromNodesWA)

        if fromNodesTF == fromNodesWA:
            console(f&#34;\t\tsame {nFromTF} fromNodes&#34;)
        else:
            console(
                f&#34;\t\tfrom nodes differ: {nFromTF} in TF, {nFromWA} in WA&#34;,
                error=True,
            )
            good = False

        diffs = []

        nToChecked = 0

        for f, toNodeInfoTF in dataTF.items():
            toNodeInfoWA = dataWA[f]
            if type(toNodeInfoTF) is dict:
                toNodeInfoTF = {k: str(v) for (k, v) in toNodeInfoTF.items()}
            else:
                toNodeInfoTF = {x: None for x in toNodeInfoTF}

            if toNodeInfoTF != toNodeInfoWA:
                diffs.append((f, toNodeInfoTF, toNodeInfoWA))

            nToChecked += len(toNodeInfoTF)

        if len(diffs):
            good = False
            console(
                f&#34;\t\tdifferences in toNodes for {len(diffs)} fromNodes&#34;, error=True
            )

            for f, toNodeInfoTF, toNodeInfoWA in sorted(diffs)[0:10]:
                console(f&#34;\t\t\tfromNode {f}&#34;, error=True)

                toNodesTF = set(toNodeInfoTF)
                toNodesWA = set(toNodeInfoWA)

                nToTF = len(toNodesTF)
                nToWA = len(toNodesWA)

                if toNodesTF == toNodesWA:
                    console(f&#34;\t\t\tsame {nToTF} toNodes&#34;)
                else:
                    console(
                        f&#34;\t\t\ttoNodes differ: {nToTF} in TF, {nToWA} in WA&#34;,
                        error=True,
                    )
                for t in toNodesTF | toNodesWA:
                    doCompare = True
                    if t not in toNodesTF:
                        console(f&#34;\t\t\t\ttoNode {t} not in TF&#34;, error=True)
                        doCompare = False
                    else:
                        valTF = toNodeInfoTF[t]

                    if t not in toNodesWA:
                        console(f&#34;\t\t\t\ttoNode {t} not in WA&#34;, error=True)
                        doCompare = False
                    else:
                        valWA = toNodeInfoWA[t]

                    if doCompare:
                        if valTF == valWA:
                            console(
                                f&#34;\t\t\t\ttoNode{t} values agree: {repr(valTF)}&#34;
                            )
                        else:
                            console(
                                f&#34;\t\t\t\ttoNode{t} values differ: &#34;
                                f&#34;TF: {repr(valTF)} WA: {repr(valWA)}&#34;,
                                error=True,
                            )

        console(f&#34;\t{rep(good)} - {nToChecked} toNodes checked&#34;, error=not good)

        if not good:
            allGood = False

    console(f&#34;{rep(allGood)} - whether all edges agree&#34;)

    return allGood</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testElements"><code class="name flex">
<span>def <span class="ident">testElements</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the elements.</p>
<p>We test the annotations representing elements/processing instructions
and check whether they correspond 1-1 to the non-slot nodes in the TF
dataset.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all these tests succeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L979-L1094" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testElements(self):
    &#34;&#34;&#34;Test the elements.

    We test the annotations representing elements/processing instructions
    and check whether they correspond 1-1 to the non-slot nodes in the TF
    dataset.

    Returns
    -------
    boolean
        Whether all these tests succeed.
    &#34;&#34;&#34;
    F = self.F
    fotypev = self.fotypev
    eoslots = self.eoslots
    skipMeta = self.skipMeta
    is_metav = self.is_metav
    annotations = self.testAnnotations

    console(&#34;Testing the elements ...&#34;)

    nElementsTF = 0
    nPisTF = 0

    for n in range(F.otype.maxSlot + 1, F.otype.maxNode + 1):
        nType = fotypev(n)
        isPi = nType.startswith(&#34;?&#34;)

        if isPi:
            nPisTF += 1

        slots = eoslots(n)
        b = slots[0]
        e = slots[-1]

        if skipMeta and (is_metav(b) or is_metav(e)):
            continue
        else:
            if not isPi:
                nElementsTF += 1

    nElementsWA = sum(1 if a[1] == &#34;element&#34; else 0 for a in annotations)
    nPisWA = sum(1 if a[1] == &#34;pi&#34; else 0 for a in annotations)

    eGood = self.compare(nElementsTF, nElementsWA)
    console(
        f&#34;{rep(eGood)} - whether the amounts of elements and nodes agree&#34;,
        error=not eGood,
    )

    console(&#34;Testing the processing instructions ...&#34;)

    pGood = self.compare(nPisTF, nPisWA)
    console(
        f&#34;{rep(pGood)} - whether the amounts of processing instructions agree&#34;,
        error=not pGood,
    )

    console(&#34;Testing the element annotations ...&#34;)

    tfFromAid = {}

    element = 0
    pi = 0
    other = 0
    good = 0
    wrong = 0
    unmapped = 0

    for aId, kind, body, target in annotations:
        if kind == &#34;node&#34;:
            tfFromAid[target] = body

    self.tfFromAid = tfFromAid

    console(f&#34;\t{len(tfFromAid)} element annotations&#34;)

    for aId, kind, body, target in annotations:
        isElem = kind == &#34;element&#34;
        isPi = kind == &#34;pi&#34;

        if not isElem and not isPi:
            other += 1
            continue

        if isElem:
            element += 1
        else:
            pi += 1

        tag = body
        node = tfFromAid.get(aId, None)
        if node is None:
            unmapped += 1
            continue

        otype = fotypev(node)

        if isPi and tag == otype[1:] or not isPi and tag == otype:
            good += 1
        else:
            wrong += 1

    console(f&#34;\tElement : {element:&gt;5} x&#34;)
    console(f&#34;\tPi      : {pi:&gt;5} x&#34;)
    console(f&#34;\tOther   : {other:&gt;5} x&#34;)
    console(f&#34;\tGood    : {good:&gt;5} x&#34;)
    console(f&#34;\tWrong   : {wrong:&gt;5} x&#34;)
    console(f&#34;\tUnmapped: {unmapped:&gt;5} x&#34;)

    aGood = wrong == 0 and unmapped == 0
    console(
        f&#34;{rep(aGood)} - whether all element annotations are ok&#34;, error=not aGood
    )

    return aGood and eGood and pGood</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testExtra"><code class="name flex">
<span>def <span class="ident">testExtra</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the extra data for on-the-fly annotations.</p>
<p>Annotations that have been generated out of the data stored in the
<code>extra</code> parameter with which the object has been initialized, all got
the kind <code>anno</code>.</p>
<p>Now we check these annotations against the data that went into it.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all these tests succeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L1269-L1346" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testExtra(self):
    &#34;&#34;&#34;Test the extra data for on-the-fly annotations.

    Annotations that have been generated out of the data stored in the
    `extra` parameter with which the object has been initialized, all got
    the kind `anno`.

    Now we check these annotations against the data that went into it.

    Returns
    -------
    boolean
        Whether all these tests succeed.
    &#34;&#34;&#34;
    annotations = self.testAnnotations
    tfFromAid = self.tfFromAid
    extra = self.extra

    console(&#34;Testing the extra annotations ...&#34;)

    attWA = []

    for aId, kind, body, target in annotations:
        if kind != &#34;anno&#34;:
            continue
        node = tfFromAid[target]
        att, value = body.split(&#34;=&#34;, 1)
        attWA.append((node, att, value))

    attWA = sorted(attWA)

    attEX = []

    for feat, featData in extra.items():
        for n, value in featData.items():
            attEX.append((n, feat, value))

    attEX = sorted(attEX)

    console(f&#34;\t{len(attEX)} extra feature values&#34;)
    console(f&#34;\t{len(attWA)} extra annotations&#34;)

    good = attWA == attEX

    def showData(tuples, isin, isout):
        data = {}

        for n, f, v in tuples:
            data.setdefault(f, {})[n] = v

        for f in sorted(data):
            fData = data[f]
            console(
                f&#34;\t{isin}: {f} misses {len(fData)} annotations in {isout}&#34;,
                error=True,
            )
            for n in sorted(fData.keys())[0:3]:
                console(f&#34;\t\t\t{n:&gt;7} = {fData[n]}&#34;, error=True)

    if not good:
        attWASet = set(attWA)
        attEXSet = set(attEX)

        onlyWA = attWASet - attEXSet
        onlyEX = attEXSet - attWASet

        if len(onlyWA):
            showData(onlyWA, &#34;WA&#34;, &#34;EX&#34;)
        else:
            console(&#34;\tWA: All extra annotations derive from the extra data&#34;)
        if len(onlyEX):
            showData(onlyEX, &#34;EX&#34;, &#34;WA&#34;)
        else:
            console(&#34;\tEX: All extra data ended up as annotations&#34;)

    console(f&#34;{rep(good)} - whether the extra annotations agree&#34;, error=not good)

    return good</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testSetup"><code class="name flex">
<span>def <span class="ident">testSetup</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Prepare the tests.</p>
<p>We read the WATM dataset and store the tokens in member <code>testTokens</code>
and the annotations in the member <code>testAnnotations</code>.
We unpack targets if they contain structured information.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L896-L943" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testSetup(self):
    &#34;&#34;&#34;Prepare the tests.

    We read the WATM dataset and store the tokens in member `testTokens`
    and the annotations in the member `testAnnotations`.
    We unpack targets if they contain structured information.
    &#34;&#34;&#34;
    textFiles = self.textFiles
    annoFiles = self.annoFiles

    tokenFiles = []

    for textFile in textFiles:
        with open(textFile) as fh:
            text = json.load(fh)
            tokens = text[&#34;_ordered_segments&#34;]
            tokenFiles.append(tokens)

    self.testTokens = tokenFiles

    annotations = []

    for annoFile in annoFiles:
        with open(annoFile) as fh:
            annos = json.load(fh)

            for aId, (kind, ns, body, target) in annos.items():
                if &#34;-&gt;&#34; in target:
                    parts = target.split(&#34;-&gt;&#34;, 1)
                else:
                    parts = [target]

                newParts = []

                for part in parts:
                    if &#34;-&#34; in part:
                        file, part = part.split(&#34;:&#34;, 1)
                        start, end = part.split(&#34;-&#34;, 1)
                        part = (int(file), int(start), int(end))

                    newParts.append(part)

                target = newParts[0] if len(newParts) == 1 else tuple(newParts)

                annotations.append((aId, kind, body, target))

    annotations = sorted(annotations)
    self.testAnnotations = annotations</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.testText"><code class="name flex">
<span>def <span class="ident">testText</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the text.</p>
<p>We test the number of tokens and the equality of the resulting text:
whether the TF and WATM datasets agree on it.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all these tests succeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L945-L977" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def testText(self):
    &#34;&#34;&#34;Test the text.

    We test the number of tokens and the equality of the resulting text:
    whether the TF and WATM datasets agree on it.

    Returns
    -------
    boolean
        Whether all these tests succeed.
    &#34;&#34;&#34;
    F = self.F
    skipMeta = self.skipMeta
    is_metav = self.is_metav
    tokenFiles = self.testTokens
    texts = self.texts

    console(&#34;Testing the text ...&#34;)

    nTokensTF = sum(
        0 if skipMeta and is_metav(s) else 1 for s in range(1, F.otype.maxSlot + 1)
    )
    nTokensWA = sum(len(tokens) for tokens in tokenFiles)
    nGood = self.compare(nTokensTF, nTokensWA)
    console(f&#34;{rep(nGood)} - whether the amounts of tokens agree&#34;, error=not nGood)

    textWA = &#34;&#34;.join(&#34;&#34;.join(tokens) for tokens in tokenFiles)
    textTF = &#34;&#34;.join(&#34;&#34;.join(text) for text in texts)

    tGood = self.strEqual(wa=textWA, tf=textTF)
    console(f&#34;{rep(tGood)} - whether the text is the same&#34;, error=not tGood)

    return nGood and tGood</code></pre>
</details>
</dd>
<dt id="tf.convert.watm.WATM.writeAll"><code class="name flex">
<span>def <span class="ident">writeAll</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Write text and annotation data to disk.</p>
<p>The data will be written as JSON files.
When the annotation data grows larger than a certain threshold, it will be
divided over several files.</p>
<p>The annotations are sorted by annotation id.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L670-L778" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def writeAll(self):
    &#34;&#34;&#34;Write text and annotation data to disk.

    The data will be written as JSON files.
    When the annotation data grows larger than a certain threshold, it will be
    divided over several files.

    The annotations are sorted by annotation id.
    &#34;&#34;&#34;

    # text files

    error = self.error

    if error:
        console(&#34;Cannot run because of an earlier error&#34;, error=True)

    app = self.app
    texts = self.texts
    annos = self.annos

    baseDir = self.repoLocation
    relative = app.context.relative
    version = app.version
    wRelative = REL_RE.sub(f&#34;/{TT_NAME}/{version}/&#34;, relative, count=1)
    resultDir = f&#34;{baseDir}{wRelative}&#34;

    textFiles = []
    self.textFiles = textFiles

    initTree(resultDir, fresh=True)

    total = 0

    for i, text in enumerate(texts):
        textFile = f&#34;{resultDir}/text-{i}.json&#34;
        textFiles.append(textFile)
        nText = len(text)
        total += nText

        with open(textFile, &#34;w&#34;) as fh:
            json.dump(
                dict(_ordered_segments=text), fh, ensure_ascii=False, indent=1
            )

        console(f&#34;Text file {i:&gt;4}: {nText:&gt;8} segments to {textFile}&#34;)

    nTextFiles = len(textFiles)
    sep = &#34;&#34; if nTextFiles == 1 else &#34;s&#34;
    console(f&#34;Text files all: {total:&gt;8} segments to {nTextFiles} file{sep}&#34;)

    # annotation files

    annoStore = {}

    for kind, aId, ns, body, target in annos:
        annoStore[aId] = (kind, ns, body, target)

    aIdSorted = sorted(annoStore.keys())

    annoFile = f&#34;{resultDir}/anno.tsv&#34;

    if False:
        with open(annoFile, &#34;w&#34;) as fh:
            for aId in aIdSorted:
                kind, ns, body, target = annoStore[aId]
                fh.write(f&#34;{aId}\t{kind}\t{ns}\t{body}\t{target}\n&#34;)

    thisAnnoStore = {}
    thisA = 1
    annoFiles = []
    self.annoFiles = annoFiles

    LIMIT = 400000
    j = 0
    total = 0

    def writeThis():
        annoFile = f&#34;{resultDir}/anno-{thisA:&gt;01}.json&#34;
        annoFiles.append(annoFile)

        with open(annoFile, &#34;w&#34;) as fh:
            json.dump(thisAnnoStore, fh, ensure_ascii=False, indent=1)

        console(f&#34;Anno file {i:&gt;4}: {j:&gt;8} annotations written to {annoFile}&#34;)

    for aId in aIdSorted:
        if j &gt;= LIMIT:
            writeThis()
            thisA += 1
            thisAnnoStore = {}
            total += j
            j = 0

        thisAnnoStore[aId] = annoStore[aId]
        j += 1

    if len(thisAnnoStore):
        writeThis()
        total += j

    if len(annos) != total:
        console(f&#34;Sum of batches : {total:&gt;8}&#34;)
        console(f&#34;All annotations: {len(annoStore):&gt;8}&#34;)
        console(&#34;Mismatch in number of annotations&#34;, error=True)

    nAnnoFiles = len(annoFiles)
    sep = &#34;&#34; if nAnnoFiles == 1 else &#34;s&#34;
    console(f&#34;Anno files all: {total:&gt;8} annotations to {nAnnoFiles} file{sep}&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="tf.convert.watm.WATMS"><code class="flex name class">
<span>class <span class="ident">WATMS</span></span>
<span>(</span><span>org, repo, backend, nsOrig, skipMeta=False, extra={})</span>
</code></dt>
<dd>
<div class="desc"><p>Export corpora that are divided over multiple TF datasets.</p>
<p>We set up and run WATM objects for each TF dataset, and generate results
for them separately.</p>
<p>We assume that all corpora have been generated by the same method and originate
from the same original format.</p>
<p>They must reside in the same repository, in adjacent directories under the <code><a title="tf" href="../index.html">tf</a></code>
top-level directory of the repo.</p>
<p>Collect the parameters for the WATM machinery.</p>
<p>We will initialize many <code><a title="tf.convert.watm.WATM" href="#tf.convert.watm.WATM">WATM</a></code> objects with mostly the same parameters.
These are collected when we initialize this object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>org</code></strong> :&ensp;<code>string</code></dt>
<dd>The organization of all TF datasets.</dd>
<dt><strong><code>repo</code></strong> :&ensp;<code>string</code></dt>
<dd>The repo of all TF datasets.</dd>
<dt><strong><code>backend</code></strong> :&ensp;<code>string</code></dt>
<dd>The backend of all TF datasets.</dd>
<dt><strong><code>nsOrig</code></strong> :&ensp;<code>string</code></dt>
<dd>The original namespace of all TF datasets.
See <code><a title="tf.convert.watm.WATM" href="#tf.convert.watm.WATM">WATM</a></code>.</dd>
<dt><strong><code>skipMeta</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>See <code><a title="tf.convert.watm.WATM" href="#tf.convert.watm.WATM">WATM</a></code>.</dd>
<dt><strong><code>extra</code></strong> :&ensp;<code>dictionary</code>, optional <code>{}</code></dt>
<dd>See <code><a title="tf.convert.watm.WATM" href="#tf.convert.watm.WATM">WATM</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L1514-L1597" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class WATMS:
    &#34;&#34;&#34;Export corpora that are divided over multiple TF datasets.

    We set up and run WATM objects for each TF dataset, and generate results
    for them separately.

    We assume that all corpora have been generated by the same method and originate
    from the same original format.

    They must reside in the same repository, in adjacent directories under the `tf`
    top-level directory of the repo.
    &#34;&#34;&#34;

    def __init__(self, org, repo, backend, nsOrig, skipMeta=False, extra={}):
        &#34;&#34;&#34;Collect the parameters for the WATM machinery.

        We will initialize many `WATM` objects with mostly the same parameters.
        These are collected when we initialize this object.

        Parameters
        ----------
        org: string
            The organization of all TF datasets.
        repo: string
            The repo of all TF datasets.
        backend: string
            The backend of all TF datasets.
        nsOrig: string
            The original namespace of all TF datasets.
            See `tf.convert.watm.WATM`.
        skipMeta: boolean, optional False
            See `tf.convert.watm.WATM`.
        extra: dictionary, optional {}
            See `tf.convert.watm.WATM`.
        &#34;&#34;&#34;
        self.org = org
        self.repo = repo
        self.backend = backend
        self.nsOrig = nsOrig
        self.skipMeta = skipMeta
        self.extra = extra

        repoDir = ex(f&#34;~/{backend}/{org}/{repo}&#34;)
        tfDir = f&#34;{repoDir}/tf&#34;
        docs = dirContents(tfDir)[1]
        console(f&#34;Found {len(docs)} docs in {tfDir}&#34;)
        self.docs = docs

    def produce(self, doc=None):
        &#34;&#34;&#34;Convert all relevant TF datasets.

        Parameters
        ----------
        doc: string, optional None
            Subdirectory where one of the TF datasets resides.
            If passed, only this dataset will be converted.
            Otherwise all datasets will be converted.
        &#34;&#34;&#34;
        org = self.org
        repo = self.repo
        backend = self.backend
        nsOrig = self.nsOrig
        skipMeta = self.skipMeta
        extra = self.extra
        docs = self.docs

        chosenDoc = doc

        for doc in sorted(docs, key=lambda x: (x[0], int(x[1:]))):
            if chosenDoc is not None and chosenDoc != doc:
                continue

            console(f&#34;{doc:&gt;5} ... &#34;, newline=False)
            A = use(
                f&#34;{org}/{repo}:clone&#34;,
                relative=f&#34;tf/{doc}&#34;,
                checkout=&#34;clone&#34;,
                backend=backend,
                silent=DEEP,
            )
            WA = WATM(A, nsOrig, skipMeta=skipMeta, extra=extra)
            WA.makeText()
            WA.makeAnno()
            WA.writeAll()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tf.convert.watm.WATMS.produce"><code class="name flex">
<span>def <span class="ident">produce</span></span>(<span>self, doc=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert all relevant TF datasets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>doc</code></strong> :&ensp;<code>string</code>, optional <code>None</code></dt>
<dd>Subdirectory where one of the TF datasets resides.
If passed, only this dataset will be converted.
Otherwise all datasets will be converted.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e86b712bf2cffe660d0ef010161536bb90b5840e/tf/convert/watm.py#L1562-L1597" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def produce(self, doc=None):
    &#34;&#34;&#34;Convert all relevant TF datasets.

    Parameters
    ----------
    doc: string, optional None
        Subdirectory where one of the TF datasets resides.
        If passed, only this dataset will be converted.
        Otherwise all datasets will be converted.
    &#34;&#34;&#34;
    org = self.org
    repo = self.repo
    backend = self.backend
    nsOrig = self.nsOrig
    skipMeta = self.skipMeta
    extra = self.extra
    docs = self.docs

    chosenDoc = doc

    for doc in sorted(docs, key=lambda x: (x[0], int(x[1:]))):
        if chosenDoc is not None and chosenDoc != doc:
            continue

        console(f&#34;{doc:&gt;5} ... &#34;, newline=False)
        A = use(
            f&#34;{org}/{repo}:clone&#34;,
            relative=f&#34;tf/{doc}&#34;,
            checkout=&#34;clone&#34;,
            backend=backend,
            silent=DEEP,
        )
        WA = WATM(A, nsOrig, skipMeta=skipMeta, extra=extra)
        WA.makeText()
        WA.makeAnno()
        WA.writeAll()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<p><a href="https://github.com/annotation" title="annotation on GitHub"><img src="../../tf/images/tf-small.png" alt="annotation"></a></p>
<p><a href="../../tf/index.html">tf home</a> -
<a href="../../tf/cheatsheet.html">cheat sheet</a> -
<a href="https://github.com/annotation/text-fabric" title="GitHub repo"><img src="../../tf/images/GitHub_Logo.png" alt="GitHub" width="50"></a></p>
</p>
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#the-general-idea">The general idea</a></li>
<li><a href="#the-specifics">The specifics</a><ul>
<li><a href="#format-of-the-text-files">Format of the text files</a><ul>
<li><a href="#tokens">Tokens</a><ul>
<li><a href="#tei-corpora">TEI corpora</a></li>
<li><a href="#pagexml-corpora">PageXML corpora</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#format-of-the-annotation-files">Format of the annotation files</a></li>
</ul>
</li>
<li><a href="#caveat">Caveat</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tf.convert" href="index.html">tf.convert</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tf.convert.watm.rep" href="#tf.convert.watm.rep">rep</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tf.convert.watm.WATM" href="#tf.convert.watm.WATM">WATM</a></code></h4>
<ul class="two-column">
<li><code><a title="tf.convert.watm.WATM.compare" href="#tf.convert.watm.WATM.compare">compare</a></code></li>
<li><code><a title="tf.convert.watm.WATM.makeAnno" href="#tf.convert.watm.WATM.makeAnno">makeAnno</a></code></li>
<li><code><a title="tf.convert.watm.WATM.makeText" href="#tf.convert.watm.WATM.makeText">makeText</a></code></li>
<li><code><a title="tf.convert.watm.WATM.mkAnno" href="#tf.convert.watm.WATM.mkAnno">mkAnno</a></code></li>
<li><code><a title="tf.convert.watm.WATM.strEqual" href="#tf.convert.watm.WATM.strEqual">strEqual</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testAll" href="#tf.convert.watm.WATM.testAll">testAll</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testAttributes" href="#tf.convert.watm.WATM.testAttributes">testAttributes</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testEdges" href="#tf.convert.watm.WATM.testEdges">testEdges</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testElements" href="#tf.convert.watm.WATM.testElements">testElements</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testExtra" href="#tf.convert.watm.WATM.testExtra">testExtra</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testSetup" href="#tf.convert.watm.WATM.testSetup">testSetup</a></code></li>
<li><code><a title="tf.convert.watm.WATM.testText" href="#tf.convert.watm.WATM.testText">testText</a></code></li>
<li><code><a title="tf.convert.watm.WATM.writeAll" href="#tf.convert.watm.WATM.writeAll">writeAll</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="tf.convert.watm.WATMS" href="#tf.convert.watm.WATMS">WATMS</a></code></h4>
<ul class="">
<li><code><a title="tf.convert.watm.WATMS.produce" href="#tf.convert.watm.WATMS.produce">produce</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://pure.knaw.nl/portal/en/persons/dirk-roorda">Dirk Roorda</a>
<a href="https://huc.knaw.nl"><img alt="HuC" src="../../tf/images/huc.png" width="200" alt="Humanities Cluster"></a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>